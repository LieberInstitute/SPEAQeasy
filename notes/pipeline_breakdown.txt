#################################################################################################
#################################################################################################

################
## PARAMETERS ##
################

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Define Configurable Variables
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

params.experiment = false
params.prefix = false
params.sample = false
params.strand = false
params.merge = false
params.input = false
params.unalign = false
params.reference = false
params.annotation = false
params.indexing = false
params.genotype = false
params.output = false
params.email = false
params.name = false
params.raw = false
params.ercc = false
params.large = false
params.fullCov = false
params.test = false
params.small_test = false
params.k_lm = false
params.k_sd = false

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Validate Inputs
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Sample Selection Validation
if (!params.sample || (params.sample != "single" && params.sample != "paired")) {
    exit 1, "Sample Type Not Provided or Invalid Choice. Please Provide a Valid Sample Type"
}

// Strand Selection Validation
if (!params.strand || (params.strand != "forward" && params.strand != "reverse" && params.strand != "unstranded")) {
    exit 1, "Strand Type Not Provided or Invalid Choice. Please Provide a Valid Strand Type"
}

// Reference Selection Validation
if (!params.reference) {
    exit 1, "Error: enter hg19 or hg38, mm10 for mouse, or rn6 for rat as the reference."
}
if (params.reference == "hg19" || params.reference == "hg38" ) {
    params.reference_type = "human"
}
if (params.reference == "mm10") {
    params.reference_type = "mouse"
}
if (params.reference == "rn6") {
    params.reference_type = "rat"
}

// Annotation Path Validation
if (!params.annotation) {
    params.annotations = "./Annotation"
}

// Indexing Path Validation
if (!params.indexing && !params.test) {
    params.indexing = "${params.annotations}"
}

// Genotype Path Validation
if (!params.genotype) {
    params.genotypes = "./Genotyping"
}

// Experiment/Workflow Name Validation
if (!params.name) {
    if (!params.experiment) {
        workflow.runName = "Extravaganza"
        params.experiments = "Crystalized"
    }
    if (params.experiment) {
        params.experiments = params.experiment
        workflow.runName = params.experiment
    }
}

// External Script Path Validation
params.scripts = "./scripts"

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Core Options
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

params.cores = '1'
params.ercc_cores = '8'
params.trimming_cores = '8'
params.hisat_cores = '8'
params.samtobam_cores = '8'
params.featurecounts_cores = '8'
params.alignments_cores = '8'
params.salmon_cores = '8'
params.counts_cores = '5'
params.coverage_cores = '5'
params.expressedregion_cores = '8'


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Input Path Options
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Base Input Path Options
if (!params.input && !params.test && !params.small_test) {
    if (params.input) {
        params.inputs = "${params.input}"
    }
    else {
        params.inputs = "./inputs"
    }
}

// Testing Paths for Rat
if (params.reference_type == "rat") {
    exit 1, "There are no sample files for rat. Please add sample files and then run again"
}

// Real File Test Paths (Human & Mouse)
if (params.test && !params.small_test) {
    if (params.reference_type == "mouse") {
        params.inputs = "/media/genomics/disco3/dataLieber/raw/${params.reference_type}/${params.sample}"
    }
    if (params.reference_type == "human") {
        if (params.raw) {
            params.inputs = "/media/genomics/disco3/dataLieber/raw/human/paired"
        }
        if (!params.raw) {
            params.inputs = "/media/genomics/disco3/dataLieber/human"
        }
    }
}

// Dummy File Test Paths
if (params.small_test && !params.test) {
    if (params.strand != "unstranded") {
        if (params.merge) {
            params.inputs = "./test/merge/${params.sample}/stranded"
        }
        if (!params.merge) {
            params.inputs = "./test/${params.sample}/stranded"
        }
    }
    if (params.strand == "unstranded") {
        if (params.merge) {
            params.inputs = "./test/merge/${params.sample}/unstranded"
        }
        if (!params.merge) {
            params.inputs = "./test/${params.sample}/unstranded"
        }
    }
}

// Conflicting Test Options
if (params.small_test && params.test) {
    exit 1, "You've selected 'small_test' and 'test' ... Please choose one and run again"
}

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Strand Option Parameters
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

if (!params.k_lm) {
    params.k_length_mean = 200
}
if (!params.k_sd) {
    params.k_standard_deviation = 30
}
if (params.k_lm){
    params.k_length_mean = params.k_lm
}
if (params.k_sd) {
    params.k_standard_deviation = params.k_sd
}

if (params.strand == "unstranded") {
    if (params.sample == "single") {
        params.kallisto_strand = "--single -l ${params.k_length_mean} -s ${params.k_standard_deviation}"
        params.hisat_strand = ""
        params.feature_strand = "0"
        params.trim_sample = "SE"
    }
    if (params.sample == "paired") {
        params.kallisto_strand = ""
        params.hisat_strand = ""
        params.feature_strand = "0"
        params.trim_sample = "PE"
    }
}
if (params.strand == "forward") {
    if (params.sample == "single") {
        params.kallisto_strand = "--single --fr-stranded -l ${params.k_length_mean} -s ${params.k_standard_deviation}"
        params.hisat_strand = "--rna-strandness F"
        params.feature_strand = "1"
        params.trim_sample = "SE"
    }
    if (params.sample == "paired") {
        params.kallisto_strand = "--fr-stranded"
        params.hisat_strand = "--rna-strandness FR"
        params.feature_strand = "1"
        params.trim_sample = "PE"
    }
}
if (params.strand == "reverse") {
    if (params.sample == "single") {
        params.kallisto_strand = "--single --rf-stranded -l ${params.k_length_mean} -s ${params.k_standard_deviation}"
        params.hisat_strand = "--rna-strandness R"
        params.feature_strand = "2"
        params.trim_sample = "SE"
    }
    if (params.sample == "paired") {
        params.kallisto_strand = "--rf-stranded"
        params.hisat_strand = "--rna-strandness RF"
        params.feature_strand = "2"
        params.trim_sample = "PE"
    }
}


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Base Output File Paths (Merging, Paired/Single, Stranded  Combinations)
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

if (params.output) {
    params.basedir = "${params.output}"
    params.index_out = "${params.annotations}"
}
if (!params.output) {
    params.production_baseout = "."
    params.test_baseout = "."
    if (params.test) {
        params.index_out = "${params.test_baseout}/Annotation"
        if (params.merge) {
            params.basedir = "${params.test_baseout}/results/${params.reference_type}/${params.reference}/${params.sample}/merge"
        }
        if (!params.merge) {
            params.basedir = "${params.test_baseout}/results/${params.reference_type}/${params.reference}/${params.sample}"
        }
    }
    if (!params.test) {
        params.index_out = "${params.production_baseout}/Annotation"
        if (params.merge) {
            params.basedir = "${params.production_baseout}/results/${params.reference_type}/${params.reference}/${params.sample}/merge"
        }
        if (!params.merge) {
            params.basedir = "${params.production_baseout}/results/${params.reference_type}/params.reference}${params.sample}"
        }
    }
}

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Define External Scripts
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

infer_strandness = file("${params.scripts}/step3b_infer_strandness.R")
prep_bed = file("${params.scripts}/prep_bed.R")
bed_to_juncs = file("${params.scripts}/bed_to_juncs.py")
fullCov_file = file("${params.scripts}/create_fullCov_object.R")
expressedRegions_file = file("${params.scripts}/step9-find_expressed_regions.R")

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Define Reference Paths/Scripts + Reference Dependent Parameters
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// ERCC
if (params.ercc) {
    erccidx = file("${params.annotations}/ERCC/ERCC92.idx")
}

// ERCC Concentrations
ercc_actual_conc = file("${params.annotations}/ercc_actual_conc.txt")

if (params.reference == "hg38") {
    
    // Step 3: hisat2
    params.fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh38.primary_assembly.genome.fa.gz"
    params.fa_gz = "GRCh38.primary_assembly.genome.fa.gz"
    params.fa = "GRCh38.primary_assembly.genome.fa"
    params.hisat_prefix = "hisat2_GRCh38primary"
    params.hisat_assembly = "GENCODE/GRCh38_hg38/assembly"

    // Step 4: gencode gtf
    params.gencode_gtf_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.annotation.gtf.gz"
    params.gencode_gtf_gz = "gencode.v25.annotation.gtf.gz"
    params.gencode_gtf = "gencode.v25.annotation.gtf"
    params.feature_output_prefix = "Gencode.v25.hg38"

    // Step 5: python coverage
    chr_sizes = file("${params.annotations}/chrom_sizes/hg38.chrom.sizes.gencode")

    // Step 6: salmon
    params.step6 = true
    params.tx_fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v27.transcripts.fa.gz"
    params.tx_fa_gz = "gencode.v25.transcripts.fa.gz"
    params.tx_fa = "gencode.v25.transcripts.fa"
    params.salmon_prefix = "salmon_0.8.2_index_gencode.v25.transcripts"
    params.salmon_assembly = "GENCODE/GRCh38_hg38/transcripts"

    // Step 7: Make R objects    
    junction_annotation_gencode = file("${params.annotations}/junction_txdb/junction_annotation_hg38_gencode_v25.rda")
    junction_annotation_ensembl = file("${params.annotations}/junction_txdb/junction_annotation_hg38_ensembl_v85.rda")
    junction_annotation_genes = file("${params.annotations}/junction_txdb/junction_annotation_hg38_refseq_grch38.rda")
    feature_to_tx_gencode = file("${params.annotations}/junction_txdb/feature_to_Tx_hg38_gencode_v25.rda")
    feature_to_tx_ensembl = file("${params.annotations}/junction_txdb/feature_to_Tx_ensembl_v85.rda")
    create_counts = file("${params.scripts}/create_count_objects-human.R")

    // Step 8: call variants
    params.step8 = true
    snv_bed = file("${params.genotypes}/common_missense_SNVs_hg38.bed")

}
if (params.reference == "hg19") {
    
    // Step 3: hisat2
    params.fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/GRCh37.primary_assembly.genome.fa.gz"
    params.fa_gz = "GRCh37.primary_assembly.genome.fa.gz"
    params.fa = "GRCh37.primary_assembly.genome.fa"
    params.hisat_prefix = "hisat2_GRCh37primary"
    params.hisat_assembly = "GENCODE/GRCh37_hg19/assembly"
    
    // Step 4: gencode gtf
    params.gencode_gtf_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.annotation.gtf.gz"
    params.gencode_gtf_gz = "gencode.v25lift37.annotation.gtf.gz"
    params.gencode_gtf = "gencode.v25lift37.annotation.gtf"
    params.feature_output_prefix = "Gencode.v25lift37.hg19"

    // Step 5: python coverage
    chr_sizes = file("${params.annotations}/chrom_sizes/hg19.chrom.sizes.gencode")

    // Step 6: salmon
    params.step6 = true
    params.tx_fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/GRCh37_mapping/gencode.v25lift37.transcripts.fa.gz"
    params.tx_fa_gz = "gencode.v25lift37.transcripts.fa.gz"
    params.tx_fa = "gencode.v25lift37.transcripts.fa"
    params.salmon_prefix = "salmon_0.8.2_index_gencode.v25lift37.transcripts"
    params.salmon_assembly = "GENCODE/GRCh37_hg19/transcripts"

    // Step 7: Make R objects
    junction_annotation_gencode = file("${params.annotations}/junction_txdb/junction_annotation_hg19_gencode_v25lift37.rda")
    junction_annotation_ensembl = file("${params.annotations}/junction_txdb/junction_annotation_hg19_ensembl_v75.rda")
    junction_annotation_genes = file("${params.annotations}/junction_txdb/junction_annotation_hg19_refseq_grch37.rda")
    feature_to_tx_gencode = file("${params.annotations}/junction_txdb/feature_to_Tx_hg19_gencode_v25lift37.rda")
    feature_to_tx_ensembl = file("${params.annotations}/junction_txdb/feature_to_Tx_ensembl_v75.rda")
    create_counts = file("${params.scripts}/create_count_objects-human.R")

    // Step 8: call variants
    params.step8 = true
    snv_bed = file("${params.genotypes}/common_missense_SNVs_hg19.bed")

}
if (params.reference == "mm10") {

    // Step 3: hisat2
    params.fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M11/GRCm38.primary_assembly.genome.fa.gz"
    params.fa_gz = "GRCm38.primary_assembly.genome.fa.gz"
    params.fa = "GRCm38.primary_assembly.genome.fa"
    params.hisat_prefix = "GRCm38_mmhisat2_GRCm38primary"
    params.hisat_assembly = "GENCODE/GRCm38_mm10/assembly"

    // Step 4: gencode gtf
    params.gencode_gtf_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M11/gencode.vM11.annotation.gtf.gz"
    params.gencode_gtf_gz = "gencode.vM11.annotation.gtf.gz"
    params.gencode_gtf = "gencode.vM11.annotation.gtf"
    params.feature_output_prefix = "Gencode.M11.mm10"

    // Step 5: python coverage
    chr_sizes = file("${params.annotations}/chrom_sizes/mm10.chrom.sizes.gencode")
    
    // Step 6: salmon
    params.step6 = true
    params.tx_fa_link = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M11/gencode.vM11.transcripts.fa.gz"
    params.tx_fa_gz = "gencode.vM11.transcripts.fa.gz"
    params.tx_fa = "gencode.vM11.transcripts.fa"
    params.salmon_prefix = "salmon_0.8.2_index_gencode.vM11.transcripts"
    params.salmon_assembly = "GENCODE/GRCm38_mm10/transcripts"

    // Step 7: Make R objects
    junction_annotation_gencode = file("${params.annotations}/junction_txdb/junction_annotation_mm10_gencode_vM11.rda")
    junction_annotation_ensembl = file("${params.annotations}/junction_txdb/junction_annotation_mm10_ensembl_v86.rda")
    create_counts = file("${params.scripts}/create_count_objects-mouse.R")

    // Step 8: call variants
    params.step8 = false
}
if (params.reference == "rn6") {

    // Step 3: hisat2
    params.fa_link = "ftp://ftp.ensembl.org/pub/release-86/fasta/rattus_norvegicus/dna/Rattus_norvegicus.Rnor_6.0.dna.toplevel.fa.gz"
    params.fa_gz = "Rattus_norvegicus.Rnor_6.0.dna.toplevel.fa.gz"
    params.fa = "Rattus_norvegicus.Rnor_6.0.dna.toplevel.fa"
    params.hisat_prefix = "hisat2_Rnor6.0toplevel"
    params.hisat_assembly = "ensembl/Rnor_6.0"
    
    
    // Step 4: gencode gtf (ensembl for rn6)
    params.gencode_gtf_link = "ftp://ftp.ensembl.org/pub/release-86/gtf/rattus_norvegicus/Rattus_norvegicus.Rnor_6.0.86.gtf.gz"
    params.gencode_gtf_gz = "Rattus_norvegicus.Rnor_6.0.86.gtf.gz"
    params.gencode_gtf = "Rattus_norvegicus.Rnor_6.0.86.gtf"
    params.feature_output_prefix = "Rnor_6.0.86"

    // Step 5: python coverage
    chr_sizes = file("${params.annotations}/chrom_sizes/rn6.chrom.sizes.ensembl")
    
    // Step 6: Salmon
    params.step6 = false

    //Step 8: call variants    
    params.step8 = false

    // Step 7: Make R Objects
    junction_annotation_ensembl = file("${params.annotations}/junction_txdb/junction_annotation_rn6_ensembl_v86.rda")
    create_counts = file("${params.scripts}/create_count_objects-rat.R")
}

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Define the Prefix Functions
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

def get_merging_prefix = { file -> file.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_read1_A1s2o2s1A" - "_read2_A1s2o2s1A" }

def get_prefix = { file -> file.name.toString().tokenize('.')[0] }

def get_paired_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_1_A1s2o2s1A" - "_2_A1s2o2s1A" }

def get_summary_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_summary_A1s2o2s1A" }

def get_summary_paired_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_summary_A1s2o2s1A" + "_A1s2o2s1A" - "_1_A1s2o2s1A" - "_2_A1s2o2s1A" }

def get_TR_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_TR_A1s2o2s1A" }

def get_TR_paired_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_TR_A1s2o2s1A" + "_A1s2o2s1A" - "_1_A1s2o2s1A" - "_2_A1s2o2s1A" }

def get_TNR_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_TNR_A1s2o2s1A" } 

def get_TNR_paired_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_TNR_A1s2o2s1A" + "_A1s2o2s1A" - "_1_A1s2o2s1A" - "_2_A1s2o2s1A" }

def get_single_trimmed_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_trimmed_A1s2o2s1A" }

def get_paired_trimmed_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_reverse_paired_A1s2o2s1A" - "_reverse_unpaired_A1s2o2s1A" - "_forward_paired_A1s2o2s1A" - "_forward_unpaired_A1s2o2s1A" + "_A1s2o2s1A" - "_trimmed_A1s2o2s1A" }

def get_hisat_prefix = { file -> file.name.toString().tokenize('.')[0] + "_A1s2o2s1A" - "_hisat_out_A1s2o2s1A" }


#################################################################################################
#################################################################################################

################
## PARAMETERS ##
################


##########################################
##           pipeline rna-seq           ##
##       Ia: Download Assembly FA       ##
##########################################

Inputs:
[
    Channel: { assembly_fa_trigger }
      - this channel checks for the asembly fa
      - flows into a process trigger that filters for file presence
      - if file not present, process triggered
]

Process:
[
    params.hisat_idx_output = "${params.index_out}/${params.hisat_assembly}"

    Channel
      .fromPath("${params.hisat_idx_output}/fa/*.fa")
      .set{ assembly_fa_trigger }
    assembly_fa_trigger.count().filter{ it == 0 }.set{ assembly_fa_download }

    process pullGENCODEassemblyfa {

        echo true
        tag "Downloading Assembly FA File: ${params.fa}"
        publishDir "${params.hisat_idx_output}/fa",'mode':'copy'

        input:
        val(assembly_fa_val) from assembly_fa_download

        output:
        file("${params.fa}") into reference_fa

        script:
        idx_file_link = "${params.fa_link}"
        idx_gz = "${params.fa_gz}"
        idx = "${params.fa}"
        """
        wget $idx_file_link
        gunzip $idx_gz
        """
    }

    Channel
      .fromPath("${params.hisat_idx_output}/fa/${params.fa}")
      .mix(reference_fa)
      .toSortedList()
      .flatten()
      .distinct()
      .into{ reference_assembly; variant_assembly }
]

Outputs:
[
    Channel: reference_fa
      - this channel outputs the file from the pull process

    Channel: { reference_assembly; variant_assembly }
      - this channel reads in the fa file from the target location
      - if the pull happened or not, the file should be present
      - channel is mixed with fa pull, even if pull never happened
      - channel is flattened and filtered for distinct values 
      - channel is mixed so that the channel is "dependent" on process completion
]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes
]


##########################################
##           pipeline rna-seq           ##
##        Ib: Build HISAT Index         ##
##########################################


Inputs:
[

]

Process:
[

    Channel
      .fromPath("${params.hisat_idx_output}/index/${params.hisat_prefix}.*.ht2")
      .set{ hisat_builds_trigger }

    hisat_builds_trigger.count().filter{ it < 8 }.set{ hisat_trigger_build }

    process buildHISATindex {

        echo true
        tag "Building HISAT2 Index: ${params.hisat_prefix}"
        publishDir "${params.hisat_idx_output}/index",mode:'copy'

        input:
        val(hisat_trigger_val) from hisat_trigger_build
        file idx from reference_assembly 

        output:
        file("${params.hisat_prefix}.*") into hisat_index_built

        script:
        prefix = "${params.hisat_prefix}"
        """
        hisat2-build -p 3 $idx $prefix
        """
    }

    Channel
      .fromPath("${params.hisat_idx_output}/index/${params.hisat_prefix}.*.ht2")
      .mix(hisat_index_built)
      .toSortedList()
      .flatten()
      .distinct()
      .toSortedList()
      .set{ hisat_index }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes
]


##########################################
##           pipeline rna-seq           ##
##    IIa: Download GENCODE GTF File    ##
##########################################


Inputs:
[

]

Process:
[

    params.gencode_gtf_out = "${params.index_out}/RSeQC/${params.reference}"

    Channel
      .fromPath("${params.gencode_gtf_out}/gtf/*.gtf")
      .set{ gencode_gtf_download_trigger }

    gencode_gtf_download_trigger.count().filter{ it == 0 }.set{ gencode_gtf_trigger_download }

    process pullGENCODEgtf {

        echo true
        tag "Downloading GTF File: ${params.gencode_gtf}"
        publishDir "${params.gencode_gtf_out}/gtf",mode:'copy'

        input:
        val(gencode_gtf_bed_val) from gencode_gtf_trigger_download

        output:
        file("${params.gencode_gtf}") into gencode_gtf_downloaded

        script:
        gencode_gtf_link = "${params.gencode_gtf_link}"
        gencode_gtf_gz = "${params.gencode_gtf_gz}"
        """
        wget $gencode_gtf_link
        gunzip $gencode_gtf_gz
        """
    }

    Channel
      .fromPath("${params.gencode_gtf_out}/gtf/${params.gencode_gtf}")
      .mix(gencode_gtf_downloaded)
      .toSortedList()
      .flatten()
      .distinct()
      .into{ gencode_gtf; create_counts_gtf; gencode_feature_gtf }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes



##########################################
##           pipeline rna-seq           ##
##          IIb: Build Bed File         ##
##########################################

Inputs:
[

]

Process:
[

    Channel
      .fromPath("${params.gencode_gtf_out}/bed/*")
      .set{ gencode_gtf_bed_trigger }

    gencode_gtf_bed_trigger.count().filter{ it == 0 }.set{ gencode_gtf_trigger_bed }

    process buildPrepBED {

        echo true
        tag "Building Bed File: ${params.reference}"
        publishDir "${params.gencode_gtf_out}/bed",mode:'copy'

        input:
        val(gencode_gtf_bed_val) from gencode_gtf_trigger_bed
        file gencode_gtf from gencode_gtf
        file prep_bed from prep_bed

        output:
        file("${name}.bed") into bedfile_built

        shell:
        name = "${params.reference}"
        '''
        Rscript !{prep_bed} -f !{gencode_gtf} -n !{name}
        '''
    }

    Channel
      .fromPath("${params.gencode_gtf_out}/bed/*")
      .mix(bedfile_built)
      .toSortedList()
      .flatten()
      .distinct()
      .set{ bedfile }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      IIIa: Download Salmon TX FA     ##
##########################################

Inputs:
[

]

Process:
[

    params.salmon_idx_output = "${params.index_out}/${params.salmon_assembly}"

    Channel
      .fromPath("${params.salmon_idx_output}/fa/*.fa")
      .set{ transcript_fa_download }

    transcript_fa_download.count().filter{ it == 0 }.set{ transcript_download_trigger }

    process pullGENCODEtranscripts {

        echo true
        tag "Downloading TX FA File: ${params.tx_fa}"
        publishDir "${params.salmon_idx_output}/fa",mode:'copy'

        input:
        val(transcript_trigger_val) from transcript_download_trigger

        output:
        file("${params.tx_fa}") into tx_fa_downloaded

        script:
        tx_fa_link = "${params.tx_fa_link}"
        tx_fa_gz = "${params.tx_fa_gz}"
        tx_fa = "${params.tx_fa}"
        """
        wget $tx_fa_link
        gunzip $tx_fa_gz
        """
    }

    Channel
      .fromPath("${params.salmon_idx_output}/fa/${params.tx_fa}")
      .mix(tx_fa_downloaded)
      .toSortedList()
      .flatten()
      .distinct()
      .set{ transcript_fa }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##       IIIb: Build Salmon Index       ##
##########################################

Inputs:
[

]

Process:
[
    Channel
      .fromPath("${params.salmon_idx_output}/salmon/${params.salmon_prefix}/*")
      .set{ salmon_build_trigger }

    salmon_build_trigger.count().filter{ it == 0 }.set{ salmon_trigger_build}

    process buildSALMONindex {

        echo true
        tag "Building Salmon Index: ${params.salmon_prefix}"
        publishDir "${params.salmon_idx_output}/salmon",mode:'copy'

        input:
        file tx_file from transcript_fa
        val(salmon_trigger_val) from salmon_trigger_build

        output:
        file("${params.salmon_prefix}") into salmon_index_built

        script:
        salmon_idx = "${params.salmon_prefix}"
        """
        salmon index -t $tx_file -i $salmon_idx -p 1 --type quasi -k 31
        """
    }

    Channel
      .fromPath("${params.salmon_idx_output}/salmon/${params.salmon_prefix}/*")
      .mix(salmon_index_built)
      .toSortedList()
      .flatten()
      .distinct()
      .toSortedList()
      .set{ salmon_index }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##          A: File Merging (O)         ##
##########################################


Inputs:
[

]

Process:
[

    Channel
      .fromPath("${params.inputs}/*.fastq.gz")
      .toSortedList()
      .flatten()
      .map{file -> tuple(get_merging_prefix(file), file) }
      .groupTuple()
      .ifEmpty{ error "Could not find file pairs for merging"}
      .set{ unmerged_pairs }

    process sampleMerging {
      
        echo true
        tag "prefix: $merging_prefix | Sample Pair: [ $unmerged_pair ]"
        publishDir "${params.basedir}/merged_fastq",mode:'copy'

        input:
        set val(merging_prefix), file(unmerged_pair) from unmerged_pairs

        output:
        file "*.fastq.gz" into ercc_merged_inputs, fastqc_merged_inputs

        script:
        read1 = "${merging_prefix}_read1.fastq"
        read2 = "${merging_prefix}_read2.fastq"
        """
        gunzip $unmerged_pair
        cat $read1 $read2 > ${merging_prefix}.fastq
        gzip ${merging_prefix}.fastq
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      B: ERCC Quality Analysis (O)    ##
##########################################

Inputs:
[

]

Process:
[
    if (params.merge) {

        if (params.sample == "single") {

            ercc_merged_inputs
              .flatten()
              .map{ file -> tuple(get_prefix(file), file) }
              .ifEmpty{ error "Could not find Channel for Merged Single Sample Files for ERCC" }
              .set{ ercc_inputs }
        }
        if (params.sample == "paired") {

            ercc_merged_inputs
              .flatten()
              .map{file -> tuple(get_paired_prefix(file), file) }
              .groupTuple()
              .ifEmpty{ error "Could not find Channel for Merged Paired Sample Files for ERCC" }
              .set{ ercc_inputs }
        }
    }
    if (!params.merge) {

        if (params.sample == "single") {

            Channel
              .fromPath("${params.inputs}/*.fastq.gz")
              .map{ file -> tuple(get_prefix(file), file) }
              .ifEmpty{ error "Could not Find Unmerged Sample Files for ERCC"}
              .set{ ercc_inputs }
        }
        if (params.sample == "paired") {

            Channel
              .fromPath("${params.inputs}/*.fastq.gz")
              .map{ file -> tuple(get_paired_prefix(file), file) }
              .groupTuple()
              .ifEmpty{ error "Could not Find Unmerged Sample Files for ERCC"}
              .set{ ercc_inputs }
        }
    }

    process sampleERCC {
     
        echo true
        tag "Prefix: $ercc_prefix | Sample: [ $ercc_input ]"
        publishDir "${params.basedir}/ercc/${ercc_prefix}",mode:'copy'

        input:
        file erccidx from erccidx
        set val(ercc_prefix), file(ercc_input) from ercc_inputs

        output:
        file "*"
        set val("${ercc_prefix}"), file("${ercc_prefix}_abundance.tsv") into ercc_abundances

        script:
        ercc_cores = "${params.ercc_cores}"
        strand_option = "${params.kallisto_strand}"
        """
        kallisto quant -i $erccidx -t $ercc_cores -o . $strand_option $ercc_input
        cp abundance.tsv ${ercc_prefix}_abundance.tsv
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


###################
## Major Channel ##
###################


Process:
[

    if (params.merge) {

        if (params.sample == "single") {

            fastqc_merged_inputs
              .flatten()
              .map{file -> tuple(get_prefix(file), file) }
              .groupTuple()
              .ifEmpty{ error "Could not find Channel for Merged Sample Files for FastQC" }
              .into{ fastqc_untrimmed_inputs; adaptive_trimming_fastqs; manifest_creation; salmon_inputs }
        }
        if (params.sample == "paired") {

            fastqc_merged_inputs
              .flatten()
              .toSortedList()
              .flatten()
              .map{file -> tuple(get_paired_prefix(file), file) }
              .groupTuple()
              .ifEmpty{ error "Could not find Channel for Merged Sample Files for FastQC" }
              .into{ fastqc_untrimmed_inputs; adaptive_trimming_fastqs; manifest_creation; salmon_inputs }
        }
    }
    if (!params.merge) {

        if (params.sample == "single") {

            Channel
              .fromPath("${params.inputs}/*")
              .flatten()
              .map{file -> tuple(get_prefix(file), file) }
              .ifEmpty{ error "Could not Find Unmerged Untrimmed Single Sample Files for FastQC"}
              .into{ fastqc_untrimmed_inputs; adaptive_trimming_fastqs; manifest_creation; salmon_inputs }
        }
        if (params.sample == "paired") {

            Channel
              .fromPath("${params.inputs}/*")
              .flatten()
              .map{file -> tuple(get_paired_prefix(file), file) }
              .groupTuple()
              .ifEmpty{ error "Could not Find Unmerged Untrimmed Paired Sample Files for FastQC"}
              .into{ fastqc_untrimmed_inputs; adaptive_trimming_fastqs; manifest_creation; salmon_inputs }
        }
    }
]

- this channel operates as the main input generator for the rest of the pipeline
- because the merge option is not required, but acts as the first channel generated for sample files, the pipeline needs to know when to read files from other process outputs and when to generate a new input channel for the pipeline
- last main generator of sample input files through a .fromPath() channel



##########################################
##           pipeline rna-seq           ##
##    C1: Individual Sample Manifest    ##
##########################################


Inputs:
[

]

Process:
[

    process sampleIndividualManifest {

        echo true
        tag "Individual Manifest: $manifest_samples $samples_prefix > samples.manifest.${samples_prefix}"
        publishDir "${params.basedir}/manifest",mode:'copy'

        input:
        set val(samples_prefix), file(manifest_samples) from manifest_creation

        output:
        file "samples.manifest.${samples_prefix}" into individual_manifests

        script:
        """
        printf "${manifest_samples} ${samples_prefix}\n" >> "samples.manifest.${samples_prefix}"
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##         C2: Sample Manifest          ##
##########################################


Inputs:
[

]

Process:
[

    individual_manifests
      .flatten()
      .collect()
      .set{ individual_manifest_files }

    process sampleManifest {

        echo true
        tag "Aggregate Manifest: $individual_manifests > samples.manifest"
        publishDir "${params.basedir}/manifest",mode:'copy'

        input:
        file individual_manifests from individual_manifest_files

        output:
        file "samples.manifest" into counts_samples_manifest, fullCov_samples_manifest

        script:
        """
        cat ${individual_manifests} > "samples.manifest"
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      1: FastQC Quality Analysis      ##
##########################################


Inputs:
[

]

Process:
[

    process sampleQualityUntrimmed {
      
        echo true
        tag "Prefix: $untrimmed_prefix | Sample: [ $fastqc_untrimmed_input ]"
        publishDir "${params.basedir}/FastQC/Untrimmed",mode:'copy'

        input:
        set val(untrimmed_prefix), file(fastqc_untrimmed_input) from fastqc_untrimmed_inputs 

        output:
        file "*"
        file "*_summary.txt" into quality_reports, count_objects_quality_reports
        file "*_fastqc_data.txt" into count_objects_quality_metrics

        script:
        if (params.sample == "single") {
            copy_command = "cp ${untrimmed_prefix}_fastqc/summary.txt ${untrimmed_prefix}_summary.txt"
            data_command = "cp ${untrimmed_prefix}_fastqc/fastqc_data.txt ${untrimmed_prefix}_fastqc_data.txt"
        }
        if (params.sample == "paired") {
            copy_command = "cp ${untrimmed_prefix}_1_fastqc/summary.txt ${untrimmed_prefix}_1_summary.txt && cp ${untrimmed_prefix}_2_fastqc/summary.txt ${untrimmed_prefix}_2_summary.txt"
            data_command = "cp ${untrimmed_prefix}_1_fastqc/fastqc_data.txt ${untrimmed_prefix}_1_fastqc_data.txt && cp ${untrimmed_prefix}_2_fastqc/fastqc_data.txt ${untrimmed_prefix}_2_fastqc_data.txt"
        }
        """
        fastqc $fastqc_untrimmed_input --extract
        $copy_command
        $data_command
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      2a: Adaptive Trimming Filter    ##
##########################################


Inputs:
[

]

Process:
[

    if (params.sample == "single") {

        quality_reports
          .flatten()
          .map{ file -> tuple(get_summary_prefix(file), file) }
          .join(adaptive_trimming_fastqs)
          .ifEmpty{ error "Cannot Find Combined Quality and Trimming Channel for Single Adaptive Trimming" }
          .set{ adaptive_trimming_single_inputs }

        process sampleAdaptiveTrimSingleReads {

          echo true
          tag "Prefix: $single_adaptive_prefix : Sample: [ $single_adaptive_fastq | $single_adaptive_summary ]"
          publishDir "${params.basedir}/Adaptive_Trim",mode:'copy'

          input:
          set val(single_adaptive_prefix), file(single_adaptive_summary), file(single_adaptive_fastq) from adaptive_trimming_single_inputs

          output:
          file "*" into trimming_fastqs, no_trimming_fastqs

          shell:
          single_quality_report = single_adaptive_prefix.toString() + "_summary.txt"
          single_trimming_input = single_adaptive_prefix.toString() + ".fastq.gz"
          '''
          export result=$(grep "Adapter Content" !{single_quality_report} | cut -c1-4)
          if [ $result == "FAIL" ] ; then
              mv !{single_trimming_input} "!{single_adaptive_prefix}_TR.fastq.gz"
          else
              mv !{single_trimming_input} "!{single_adaptive_prefix}_TNR.fastq.gz"
          fi
          '''
        }
    }
    if (params.sample == "paired") {

        quality_reports
          .flatten()
          .map{ file -> tuple(get_summary_paired_prefix(file), file) }
          .groupTuple()
          .join(adaptive_trimming_fastqs)
          .ifEmpty{ error "Cannot Find Combined Quality and Trimming Channel for Paired Adaptive Trimming" }
          .set{ adaptive_trimming_paired_inputs }

        process sampleAdaptiveTrimPairedReads {

          echo true
          tag "Prefix: $paired_adaptive_prefix | Sample: [ $paired_adaptive_fastq | $paired_adaptive_summary ]"
          publishDir "${params.basedir}/Adaptive_Trim",mode:'copy'

          input:
          set val(paired_adaptive_prefix), file(paired_adaptive_summary), file(paired_adaptive_fastq) from adaptive_trimming_paired_inputs

          output:
          file "*" into trimming_fastqs, no_trimming_fastqs

          shell:
          quality_report_1 = paired_adaptive_prefix.toString() + "_1_summary.txt"
          quality_report_2 = paired_adaptive_prefix.toString() + "_2_summary.txt"
          trimming_input_1 = paired_adaptive_prefix.toString() + "_1.fastq.gz"
          trimming_input_2 = paired_adaptive_prefix.toString() + "_2.fastq.gz"
          adaptive_out_prefix_1 = paired_adaptive_prefix.toString() + "_1"
          adaptive_out_prefix_2 = paired_adaptive_prefix.toString() + "_2"

          '''
          export result1=$(grep "Adapter Content" !{quality_report_1} | cut -c1-4)
          export result2=$(grep "Adapter Content" !{quality_report_2} | cut -c1-4)
          if [ $result1 == "FAIL" || $result2 == "FAIL"] ; then
              cp !{trimming_input_1} "!{adaptive_out_prefix_1}_TR.fastq.gz"
              cp !{trimming_input_2} "!{adaptive_out_prefix_2}_TR.fastq.gz"
          else
              cp !{trimming_input_1} "!{adaptive_out_prefix_1}_TNR.fastq.gz"
              cp !{trimming_input_2} "!{adaptive_out_prefix_2}_TNR.fastq.gz"
          fi
          '''
        }
    }

    if (params.sample == "single") {

        trimming_fastqs
          .flatten()
          .filter{ file -> file.name.toString() =~ /_TR.*/ }
          .map{ file -> tuple(get_TR_prefix(file), file) }
          .set{ trimming_inputs }

        no_trimming_fastqs
          .flatten()
          .filter{ file -> file.name.toString() =~ /_TNR.*/ }
          .map{ file -> tuple(get_TNR_prefix(file), file) }
          .set{ no_trim_fastqs }
      }

      if (params.sample == "paired") {

        trimming_fastqs
          .flatten()
          .filter{ file -> file.name.toString() =~ /_TR.*/ }
          .toSortedList()
          .flatten()
          .map{ file -> tuple(get_TR_paired_prefix(file), file) }
          .groupTuple()
          .set{ trimming_inputs }

        no_trimming_fastqs
          .flatten()
          .filter{ file -> file.name.toString() =~ /_TNR.*/ }
          .toSortedList()
          .flatten()
          .map{ file -> tuple(get_TNR_paired_prefix(file), file) }
          .groupTuple()
          .set{ no_trim_fastqs }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##          2b: File Trimming           ##
##########################################


Inputs:
[

]

Process:
[

    process sampleTrimming {

        echo true
        tag "Prefix: $trimming_prefix | Sample: [ $trimming_input ]"
        publishDir "${params.basedir}/trimmed_fq",mode:'copy'

        input:
        set val(trimming_prefix), file(trimming_input) from trimming_inputs

        output:
        file "*.fastq.gz" into trimmed_fastqc_inputs, trimmed_hisat_inputs

        script:
        trimming_cores = "${params.trimming_cores}"
        sample_option = "${params.trim_sample}"
        if (params.sample == "single") {
            output_option = "${trimming_prefix}_trimmed.fastq.gz"
        }
        if (params.sample == "paired") {
            output_option = "${trimming_prefix}_trimmed_forward_paired.fastq.gz ${trimming_prefix}_trimmed_forward_unpaired.fastq.gz ${trimming_prefix}_trimmed_reverse_paired.fastq.gz ${trimming_prefix}_trimmed_reverse_unpaired.fastq.gz"
        }
        """
        java -Xmx512M \
        -jar /usr/local/bin/trimmomatic-0.36.jar \
        $sample_option \
        -threads $trimming_cores \
        -phred33 \
        $trimming_input \
        $output_option \
        ILLUMINACLIP:/usr/local/TruSeq2-PE.fa:2:30:10:1 \
        LEADING:3 \
        TRAILING:3 \
        SLIDINGWINDOW:4:15 \
        MINLEN:75
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##  2c: FastQC Trimmed Quality Analysis ##
##########################################


Inputs:
[

]

Process:
[

    process sampleQualityTrimmed {
      
        echo true
        tag "$fastqc_trimmed_input"
        publishDir "${params.basedir}/FastQC/Trimmed",mode:'copy'

        input:
        file fastqc_trimmed_input from trimmed_fastqc_inputs

        output:
        file "*"

        script:
        """
        fastqc $fastqc_trimmed_input --extract
        """
    }
]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      3a: Single Hisat2 Index         ##
##########################################

Inputs:
[

]

Process:
[
    trimmed_hisat_inputs
      .flatten()
      .map{ file -> tuple(get_single_trimmed_prefix(file), file) }
      .mix(no_trim_fastqs)
      .ifEmpty{ error "Single End Channel for HISAT is empty" }
      .set{ single_hisat_inputs }


    process sampleSingleEndHISAT {

      echo true
      tag "Prefix: $single_hisat_prefix | Sample: $single_hisat_input"
      publishDir "${params.basedir}/HISAT2_out",mode:'copy'

      input:
      file hisat_index from hisat_index
      set val(single_hisat_prefix), file(single_hisat_input) from single_hisat_inputs

      output:
      file "*_hisat_out.sam" into hisat_single_output
      file "*"
      file "*_align_summary.txt" into alignment_summaries

      shell:
      hisat_prefix = "${params.hisat_prefix}"
      strand = "${params.hisat_strand}"
      hisat_cores = "${params.hisat_cores}"
      '''
      hisat2 \
      -p !{hisat_cores} \
      -x !{hisat_prefix} \
      -U !{single_hisat_input} \
      -S !{single_hisat_prefix}_hisat_out.sam !{strand} --phred33 \
      2> !{single_hisat_prefix}_align_summary.txt
      '''
    }

    hisat_single_output
      .flatten()
      .map{ file -> tuple(get_hisat_prefix(file), file) }
      .set{ sam_to_bam_inputs }
}

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##       3a: Paired Hisat3 No Trim      ##
##########################################


Inputs:
[

]

Process:
[
    no_trim_fastqs
      .set{ notrim_paired_hisat_inputs }

    process samplePairedEndNoTrimHISAT {

      echo true
      tag "Prefix: $paired_notrim_hisat_prefix | Sample: [ $paired_no_trim_hisat ]"
      publishDir "${params.basedir}/HISAT2_out",mode:'copy'

      input:
      file hisatidx from hisat_index
      set val(paired_notrim_hisat_prefix), file(paired_no_trim_hisat) from notrim_paired_hisat_inputs

      output:
      file "*_hisat_out.sam" into hisat_paired_notrim_output
      file "*"
      file "*_align_summary.txt" into alignment_summaries

      shell:
      hisat_prefix = "${params.hisat_prefix}"
      strand = "${params.hisat_strand}"
      hisat_cores = "${params.hisat_cores}"
      sample_1_hisat = paired_notrim_hisat_prefix.toString() + "_1_TNR.fastq.gz"
      sample_2_hisat = paired_notrim_hisat_prefix.toString() + "_2_TNR.fastq.gz"
      if (params.unalign) {
          unaligned_opt = "--un-conc ${paired_notrim_hisat_prefix}.fastq"
      }
      if (!params.unalign) {
          unaligned_opt = ""
      }
      '''
      hisat2 \
      -p !{hisat_cores} \
      -x !{hisat_prefix} \
      -1 !{sample_1_hisat} \
      -2 !{sample_2_hisat} \
      -S !{paired_notrim_hisat_prefix}_hisat_out.sam !{strand} --phred33 \
      !{unaligned_opt} \
      2> !{paired_notrim_hisat_prefix}_align_summary.txt
      '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes




##########################################
##           pipeline rna-seq           ##
##      3b: Paired Hisat2 Trimmed       ##
##########################################


Inputs:
[

]

Process:
[

    trimmed_hisat_inputs
      .flatten()
      .map{ file -> tuple(get_paired_trimmed_prefix(file), file) }
      .groupTuple()
      .set{ trim_paired_hisat_inputs }


    process samplePairedEndTrimmedHISAT {

      echo true
      tag "Prefix: $paired_trimmed_prefix | Sample: $paired_trimmed_fastqs"
      publishDir "${params.basedir}/HISAT2_out",mode:'copy'

      input:
      file hisatidx from hisat_index
      set val(paired_trimmed_prefix), file(paired_trimmed_fastqs) from trim_paired_hisat_inputs

      output:
      file "*_hisat_out.sam" into hisat_paired_trim_output
      file "*"
      file "*_align_summary.txt" into alignment_summaries

      shell:
      hisat_prefix = "${params.hisat_prefix}"
      strand = "${params.hisat_strand}"
      hisat_cores = "${params.hisat_cores}"
      forward_paired = paired_trimmed_prefix.toString() + "_trimmed_forward_paired.fastq.gz"
      reverse_paired = paired_trimmed_prefix.toString() + "_trimmed_forward_paired.fastq"
      forward_unpaired = paired_trimmed_prefix.toString() + "_trimmed_forward_unpaired.fastq.gz"
      reverse_unpaired = paired_trimmed_prefix.toString() + "_trimmed_forward_unpaired.fastq.gz"
      if (params.unalign) {
          unaligned_opt = "--un-conc ${paired_trimmed_prefix}.fastq"
      }
      if (!params.unalign) {
          unaligned_opt = ""
      }
      '''
      hisat2 \
      -p !{hisat_cores} \
      -x !{hisat_prefix} \
      -1 !{forward_paired} \
      -2 !{reverse_paired} \
      -U !{forward_unpaired} , !{reverse_unpaired} \
      -S !{paired_trimmed_prefix}_hisat_out.sam !{strand} --phred33 \
      !{unaligned_opt} \
      2> !${paired_trimmed_prefix}_align_summary.txt
      '''
    }

    hisat_paired_notrim_output
      .mix(hisat_paired_trim_output)
      .flatten()
      .map{ file -> tuple(get_hisat_prefix(file), file) }
      .set{ sam_to_bam_inputs }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##        3b: Convert Sam to Bam        ##
##########################################

Inputs:
[

]

Process:
[

    process sampleSamtoBam {

        echo true
        tag "Prefix: $sam_to_bam_prefix | Sample: $sam_to_bam_input"
        publishDir "${params.basedir}/HISAT2_out/sam_to_bam",mode:'copy'

        input:
        set val(sam_to_bam_prefix), file(sam_to_bam_input) from sam_to_bam_inputs

        output:
        set val("${sam_to_bam_prefix}"), file("${sam_to_bam_prefix}*.sorted.bam"), file("${sam_to_bam_prefix}*.sorted.bam.bai") into infer_experiment_inputs, feature_bam_inputs, alignment_bam_inputs, junction_bam_inputs, coverage_bam_inputs, full_coverage_bams, count_objects_bam_files, variant_calls_bam

        script:
        original_bam = "${sam_to_bam_prefix}_accepted_hits.bam"
        sorted_bam = "${sam_to_bam_prefix}_accepted_hits.sorted"
        samtobam_cores = "${params.samtobam_cores}"
        """
        samtools view -bh -F 4 $sam_to_bam_input > $original_bam
        samtools sort -@ $samtobam_cores $original_bam -o ${sorted_bam}.bam
        samtools index ${sorted_bam}.bam
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##         3c: Infer Experiment         ##
##########################################

Inputs:
[

]

Process:
[

    process sampleInferExperiment {

        echo true
        tag "Prefix: $infer_prefix | Sample: $bam_file | Index: $bam_index"
        publishDir "${params.basedir}/HISAT2_out/infer_experiment",mode:'copy'

        input:
        set val(infer_prefix), file(bam_file), file(bam_index) from infer_experiment_inputs
        file bedfile from bedfile

        output:
        file "*"
        file "${infer_prefix}_experiment.txt" into infer_experiment_outputs

        shell:
        '''
        python /usr/local/bin/infer_experiment.py \
        -i !{bam_file} \
        -r !{bedfile} \
        1> !{infer_prefix}_experiment.txt \
        2> !{infer_prefix}_experiment_summary_out.txt
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##         3d: Infer Strandness         ##
##########################################


Inputs:
[

]

Process:
[

    infer_experiment_outputs
      .toSortedList()
      .set{ infer_experiment_output }

    process sampleInferStrandness {

        echo true
        tag "Sample: $infer_experiment_files"
        publishDir "${params.basedir}/HISAT2_out/infer_strandness/",mode:'copy'

        input:
        file infer_strandness from infer_strandness
        file infer_experiment_files from infer_experiment_output

        output:
        file "*"
        file "inferred_strandness_pattern.txt" into inferred_strand_coverage, inferred_strand_mean_coverage, inferred_strand_objects

        shell:
        inferred_strandness_pattern = "inferred_strandness_pattern.txt"
        '''
        Rscript !{infer_strandness} -p !{inferred_strandness_pattern}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##          4a: Feature Counts          ##
##########################################


Inputs:
[

]

Process:
[
    process sampleFeatureCounts {

        echo true
        tag "Prefix: $feature_prefix | Sample: $feature_bam | Sample Index: $feature_index"
        publishDir "${params.basedir}/Counts",mode:'copy'

        input:
        set val(feature_prefix), file(feature_bam), file(feature_index) from feature_bam_inputs
        file gencode_gtf_feature from gencode_feature_gtf

        output:
        file "*"
        file "*.counts" into sample_counts

        script:
        if (params.sample == "single") {
            sample_option = ""
        }
        if (params.sample == "paired") {
            sample_option = "-p"
        }
        feature_out = "${feature_prefix}_${params.feature_output_prefix}"
        featurecounts_cores = "${params.featurecounts_cores}"
        feature_strand = "${params.feature_strand}"
        """ 
        featureCounts \
        -s $feature_strand \
        $sample_option \
        -T $featurecounts_cores \
        -a $gencode_gtf_feature \
        -o ${feature_out}_Genes.counts \
        $feature_bam

        featureCounts \
        -s $feature_strand \
        $sample_option \
        -O \
        -f \
        -T $featurecounts_cores \
        -a $gencode_gtf_feature \
        -o ${feature_out}_Exons.counts \
        $feature_bam
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##        4b: Primary Alignments        ##
##########################################


Inputs:
[

]

Process:
[

    process samplePrimaryAlignments {

        echo true
        tag "Prefix: $alignment_prefix | Sample: [ $alignment_bam ]"
        publishDir "${params.basedir}/Counts/junction/primary_aligments",mode:'copy'

        input:
        set val(alignment_prefix), file(alignment_bam), file(alignment_index) from alignment_bam_inputs

        output:
        set val("${alignment_prefix}"), file("${alignment_prefix}.bam"), file("${alignment_prefix}.bam.bai") into primary_alignments

        script:
        alignments_cores = "${params.alignments_cores}"
        """
        samtools view -@ $alignments_cores -bh -F 0x100 $alignment_bam > ${alignment_prefix}.bam
        samtools index ${alignment_prefix}.bam
        """
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes

##########################################
##           pipeline rna-seq           ##
##            4c: Junctions             ##
##########################################


Inputs:
[

]

Process:
[

    process sampleJunctions {

        echo true
        tag "Prefix: $junction_prefix | Sample: [ $alignment_bam ]"
        publishDir "${params.basedir}/Counts/junction",mode:'copy'

        input:
        file bed_to_juncs from bed_to_juncs
        set val(junction_prefix), file(alignment_bam), file(alignment_index) from primary_alignments

        output:
        file "*"
        file("*.count") into junction_counts

        shell:
        outjxn = "${junction_prefix}_junctions_primaryOnly_regtools.bed"
        outcount = "${junction_prefix}_junctions_primaryOnly_regtools.count"
        '''
        regtools junctions extract -i 9 -o !{outjxn} !{alignment_bam}
        python !{bed_to_juncs} < !{outjxn} > !{outcount}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##             5a: Coverage             ##
##########################################

Inputs:
[

]

Process:
[

    process sampleCoverage {

        echo true
        tag "Prefix: $coverage_prefix | Infer: $inferred_strand | Sample: $sorted_coverage_bam ]"
        publishDir "${params.basedir}/Coverage/wigs",mode:'copy'

        input:
        file inferred_strand from inferred_strand_coverage
        set val(coverage_prefix), file(sorted_coverage_bam), file(sorted_bam_index) from coverage_bam_inputs
        file chr_sizes from chr_sizes

        output:
        set val("${coverage_prefix}"), file("${coverage_prefix}.wig") into wig_files

        shell:
        '''
        export coverage_strand_rule=$(cat !{inferred_strand})
        if [ $coverage_strand_rule == "none" ] ; then
            python /usr/local/bin/bam2wig.py -s !{chr_sizes} -i !{sorted_coverage_bam} -t 4000000000 -o !{coverage_prefix}
        elif [ $coverage_strand_rule == "1++,1--,2+-,2-+" ] ; then
            python /usr/local/bin/bam2wig.py -s !{chr_sizes} -i !{sorted_coverage_bam} -t 4000000000 -o !{coverage_prefix} -d "1++,1--,2+-,2-+"
        elif [ $coverage_strand_rule == "1+-,1-+,2++,2--" ] ; then
            python /usr/local/bin/bam2wig.py -s !{chr_sizes} -i !{sorted_coverage_bam} -t 4000000000 -o !{coverage_prefix} -d "1+-,1-+,2++,2--"
        elif [ $coverage_strand_rule == "++,--" ] ; then
          python /usr/local/bin/bam2wig.py -s !{chr_sizes} -i !{sorted_coverage_bam} -t 4000000000 -o !{coverage_prefix} -d "++,--"
        elif [ $coverage_strand_rule == "+-,-+" ] ; then
            python /usr/local/bin/bam2wig.py -s !{chr_sizes} -i !{sorted_coverage_bam} -t 4000000000 -o !{coverage_prefix} -d "+-,-+"
        fi
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes

##########################################
##           pipeline rna-seq           ##
##           5b: WigtoBigWig            ##
##########################################

Inputs:
[

]

Process:
[

    process sampleWigToBigWig {

        echo true
        tag "Prefix: $wig_prefix | Sample: [ $wig_file ]"
        publishDir "${params.basedir}/Coverage/BigWigs",mode:'copy'

        input:
        set val(wig_prefix), file(wig_file) from wig_files
        file chr_sizes from chr_sizes

        output:
        file "${wig_prefix}.bw" into coverage_bigwigs

        shell:
        '''
        /usr/local/bin/wigToBigWig !{wig_file} !{chr_sizes} !{wig_prefix}.bw
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes

##########################################
##           pipeline rna-seq           ##
##           5c: Mean Coverage          ##
##########################################


Inputs:
[

]

Process:
[

    coverage_bigwigs
      .flatten()
      .collect()
      .toSortedList()
      .into{ mean_coverage_bigwigs;full_coverage_bigwigs }

    process sampleMeanCoverage {

        echo true
        tag "Samples: [ $mean_coverage_bigwig ]"
        publishDir "${params.basedir}/Coverage/mean",mode:'copy'

        input:
        file inferred_strand_file from inferred_strand_mean_coverage
        file mean_coverage_bigwig from mean_coverage_bigwigs
        file chr_sizes from chr_sizes

        output:
        file "*" 
        file "mean*.bw" into mean_bigwigs, expressed_regions_mean_bigwigs

        shell:
        '''
        export coverage_strand_rule=$(cat !{inferred_strand_file})
        if [ $coverage_strand_rule == "none" ] ; then
            /usr/local/bin/wiggletools write mean.wig mean !{mean_coverage_bigwig}
            /usr/local/bin/wigToBigWig mean.wig !{chr_sizes} mean.bw
        else
            /usr/local/bin/wiggletools write mean.forward.wig mean !{mean_coverage_bigwig}
            /usr/local/bin/wigToBigWig mean.forward.wig !{chr_sizes} mean.forward.bw
            /usr/local/bin/wiggletools write mean.reverse.wig mean !{mean_coverage_bigwig}
            /usr/local/bin/wigToBigWig mean.reverse.wig !{chr_sizes} mean.reverse.bw
        fi
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##          6: Salmon TXQuant           ##
##########################################

Inputs:
[

]

Process:
[

    process sampleTXQuant {

        echo true
        tag "Prefix: $salmon_input_prefix | Sample: [ $salmon_inputs ]"
        publishDir "${params.basedir}/Salmon_tx",mode:'copy'

        input:
        file salmon_index from salmon_index
        set val(salmon_input_prefix), file(salmon_inputs) from salmon_inputs

        output:
        file "*"
        set val("${salmon_input_prefix}"), file("${salmon_input_prefix}_quant.sf") into salmon_quants

        shell:
        salmon_cores = "${params.salmon_cores}"
        salmon_index_prefix = "${params.salmon_prefix}"
        if (params.sample == "single") {
            sample_command = "-r ${salmon_input_prefix}.fastq.gz"
            if (params.strand == "unstranded" ) {
                salmon_strand = "U"
            }
            if (params.strand == "forward" ) {
                salmon_strand = "SF"
            }
            if (params.strand == "reverse" )
                salmon_strand = "SR"
            }
        if (params.sample == "paired") {
            sample_command = "-1 ${salmon_input_prefix}_1.fastq.gz -2 ${salmon_input_prefix}_2.fastq.gz"
            if (params.strand == "unstranded" ) {
                salmon_strand = "IU"
            }
            if (params.strand == "forward" ) {
                salmon_strand = "ISF"
            }
            if (params.strand == "reverse" ) {
                salmon_strand = "ISR"
            }
        }
        '''
        mkdir !{salmon_index_prefix}
        cp !{salmon_index} !{salmon_index_prefix}/.
        salmon quant \
        -i !{salmon_index_prefix} \
        -p !{salmon_cores} \
        -l !{salmon_strand} \
        !{sample_command} \
        -o !{salmon_input_prefix}
        cp !{salmon_input_prefix}/quant.sf !{salmon_input_prefix}_quant.sf
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      7a: Create Counts Objects       ##
##########################################

Inputs:
[

]

Process:
[

    count_objects_bam_files
      .flatten()
      .mix(count_objects_quality_reports)
      .mix(count_objects_quality_metrics)
      .mix(alignment_summaries)
      .mix(create_counts_gtf)
      .mix(sample_counts)
      .flatten()
      .set{ counts_objects_channel }

    if (params.reference_type == "human" || params.reference_type == "mouse") {

        counts_objects_channel
          .mix(salmon_quants)
          .set{counts_objects_channel_1}
    }
    if (params.reference_type == "rat") {

        counts_objects_channel
          .set{counts_objects_channel_1}
    }
    if (params.ercc) {
            
        counts_objects_channel_1
          .mix(ercc_abundances)
          .flatten()
          .toSortedList()
          .flatten()
          .collect()
          .toSortedList()
          .set{ counts_inputs }
    }
    if (!params.ercc) {

        counts_objects_channel_1
          .flatten()
          .toSortedList()
          .flatten()
          .collect()
          .toSortedList()
          .set{ counts_inputs }
    }

    process sampleCreateCountObjects {

        echo true
        tag "Creating Counts Objects"
        publishDir "${params.basedir}/Count_Objects",mode:'copy'

        input:
        file counts_input from counts_inputs
        file create_counts from create_counts
        file ercc_actual_conc from ercc_actual_conc
        file counts_sample_manifest from counts_samples_manifest
        file junction_annotation_genes from junction_annotation_genes
        file junction_annotation_gencode from junction_annotation_gencode
        file junction_annotation_ensembl from junction_annotation_ensembl
        file feature_to_tx_gencode from feature_to_tx_gencode
        file feature_to_tx_ensembl from feature_to_tx_ensembl

        output:
        file "*"

        shell:
        if (params.ercc) {
            ercc_bool = "TRUE"
        }
        if (!params.ercc) {
            ercc_bool = "FALSE"
        }
        if (params.sample == "paired") {
            counts_pe = "TRUE"
        }
        if (params.sample == "single") {
            counts_pe = "FALSE"
        }
        if (params.strand == "unstranded") {
            counts_strand = ""
        }
        if (params.strand == "forward") {
            counts_strand = "-s forward"
        }
        if (params.strand == "reverse") {
            counts_strand = "-s reverse"
        }
        counts_cores = "${params.counts_cores}"
        counts_reference = "${params.reference}"
        counts_experiment = "${params.experiments}"
        counts_prefix = "${params.prefix}"
        '''
        Rscript !{create_counts} -o !{counts_reference} -m . -e !{counts_experiment} -p !{counts_prefix} -l !{counts_pe} -c !{ercc_bool} -t !{counts_cores} !{counts_strand}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##      7b: Create Coverage Objects     ##
##########################################

Inputs:
[

]

Process:
[

    full_coverage_bams
      .mix(full_coverage_bigwigs)
      .flatten()
      .collect()
      .set{ full_coverage_inputs }

    process sampleCreateCoverageObjects {

        echo true
        tag "Creating Coverage Objects"
        publishDir "${params.basedir}/Coverage_Objects",mode:'copy'

        input:
        file fullCov_file from fullCov_file
        file fullCov_samples_manifest from fullCov_samples_manifest
        file full_coverage_input from full_coverage_inputs
        file inferred_strand_R_object from inferred_strand_objects

        output:
        file "*"

        shell:
        if (params.sample == "paired") {
            coverage_pe = "TRUE"
        }
        if (params.sample == "single") {
            coverage_pe = "FALSE"
        }
        coverage_cores = "${params.coverage_cores}"
        coverage_reference = "${params.reference}"
        coverage_experiment = "${params.experiment}"
        coverage_prefix = "${params.prefix}"
        coverage_fullCov = "TRUE"
        '''
        Rscript !{fullCov_file} -o !{coverage_reference} -m . -e !{coverage_experiment} -p !{coverage_prefix} -l !{coverage_pe} -f !{coverage_fullCov} -c !{coverage_cores}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes


##########################################
##           pipeline rna-seq           ##
##          8a: Call Variants           ##
##########################################


Inputs:
[

]

Process:
[

    process sampleVariantCalls {

        echo true
        tag "Prefix: $variant_bams_prefix | Sample: [ $variant_calls_bam_file, $variant_calls_bai ]"
        publishDir "${params.basedir}/Variant_Calls",mode:'copy'

        input:
        set val(variant_bams_prefix), file(variant_calls_bam_file), file(variant_calls_bai) from variant_calls_bam
        file variant_assembly from variant_assembly
        file snv_bed from snv_bed

        output:
        file "*"
        file "${variant_bams_prefix}.vcf.gz" into compressed_variant_calls
        file "${variant_bams_prefix}.vcf.gz.tbi" into compressed_variant_calls_tbi

        shell:
        snptmp = "${variant_bams_prefix}_tmp.vcf"
        snpoutgz = "${variant_bams_prefix}.vcf.gz"
        '''
        samtools mpileup \
        -l !{snv_bed} \
        -AB \
        -q0 \
        -Q13 \
        -d1000000 \
        -uf !{variant_assembly} !{variant_calls_bam_file} \
        -o !{snptmp}
        bcftools call \
        -mv \
        -Oz !{snptmp} > !{snpoutgz}
        tabix -p vcf !{snpoutgz}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes

##########################################
##           pipeline rna-seq           ##
##       8b: Merge Called Variants      ##
##########################################


Inputs:
[

]

Process:
[

    compressed_variant_calls
      .flatten()
      .collect()
      .toSortedList()
      .set{ collected_variant_calls }

    compressed_variant_calls_tbi
      .flatten()
      .collect()
      .toSortedList()
      .set{ collected_variant_calls_tbi }


    /*
     * Step 8b: Merge Variant Calls
     */

    process sampleVariantCallsMerge {

        echo true
        tag "Sample: $collected_variants"
        publishDir "${params.basedir}/Merged_Variants",mode:'copy'

        input:
        file collected_variants from collected_variant_calls
        file collected_variants_tbi from collected_variant_calls_tbi

        output:
        file "*"

        shell:
        '''
        vcf-merge !{collected_variants} | bgzip -c > mergedVariants.vcf.gz
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes

##########################################
##           pipeline rna-seq           ##
##         9: Expressed Regions         ##
##########################################

Inputs:
[

]

Process:
[

    process sampleExpressedRegions {

        echo true
        tag "Sample: $expressed_regions_mean_bigwig"
        publishDir "${params.basedir}/Expressed_Regions",mode:'copy'

        input:
        file expressedRegions_file from expressedRegions_file
        file chr_sizes from chr_sizes
        file expressed_regions_mean_bigwig from expressed_regions_mean_bigwigs

        output:
        file "*" 

        shell:
        expressedregion_cores = "${params.expressedregion_cores}"
        '''
        Rscript !{expressedRegions_file} \
        -m !{expressed_regions_mean_bigwig} \
        -o . \
        -i !{chr_sizes} \
        -c !{expressed_regions_cores}
        '''
    }

]

Outputs:
[

]

Module execution:
[
        Module is executed within the pipeline:

        1) `wget $idx_file_link`; downloads the target fa if the file is not already present

        2) `gunzip $idx_gz`; gunzip the fa file for use downstream in other processes