[["index.html", "SPEAQeasy Overview Cite SPEAQeasy R session information", " SPEAQeasy Nicholas J. Eagles Lieber Institute for Brain Development, Johns Hopkins Medical Campus Leonardo Collado-Torres Lieber Institute for Brain Development, Johns Hopkins Medical CampusCenter for Computational Biology, Johns Hopkins University lcolladotor@gmail.com Overview SPEAQeasy is a Nextflow-based Scalable RNA-seq Pipeline for Expression Analysis and Quantification. Diagram representing the “conceptual” workflow traversed by SPEAQeasy. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated here. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, preparing a particular set of annotation files occurs once and uses a cache for further runs. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. Cite SPEAQeasy We hope that SPEAQeasy will be useful for your research. Please use the following information to cite the package and the overall approach. Thank you! @article {Eagles2021, author = {Eagles, Nicholas J. and Burke, Emily E. and Leonard, Jabob and Barry, Brianna K. and Stolz, Joshua M. and Huuki, Louise and Phan, BaDoi N. and Larrios Serrato, Violeta and Guti{\\&#39;e}rrez-Mill{\\&#39;a}n, Everardo and Aguilar-Ordo{\\~n}ez, Israel and Jaffe, Andrew E. and Collado-Torres, Leonardo}, title = {SPEAQeasy: a scalable pipeline for expression analysis and quantification for R/bioconductor-powered RNA-seq analyses}, year = {2021}, doi = {10.1186/s12859-021-04142-3}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://doi.org/10.1186/s12859-021-04142-3}, journal = {BMC Bioinformatics} } This is a project by the R/Bioconductor-powered Team Data Science at the Lieber Institute for Brain Development. R session information Details on the R version used for making this book. The source code is available at LieberInstitute/SPEAQeasy. ## Load the package at the top of your script library(&quot;sessioninfo&quot;) ## Reproducibility information print(&#39;Reproducibility information:&#39;) Sys.time() proc.time() options(width = 120) session_info() ## [1] &quot;Reproducibility information:&quot; ## [1] &quot;2021-07-09 16:08:22 UTC&quot; ## user system elapsed ## 0.463 0.077 0.461 ## ─ Session info ─────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.0 (2021-05-18) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz UTC ## date 2021-07-09 ## ## ─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────── ## package * version date lib source ## bookdown 0.22 2021-04-22 [1] RSPM (R 4.1.0) ## cli 3.0.0 2021-06-30 [2] RSPM (R 4.1.0) ## digest 0.6.27 2020-10-24 [2] RSPM (R 4.1.0) ## evaluate 0.14 2019-05-28 [2] RSPM (R 4.1.0) ## htmltools 0.5.1.1 2021-01-22 [1] RSPM (R 4.1.0) ## knitr 1.33 2021-04-24 [2] RSPM (R 4.1.0) ## magrittr 2.0.1 2020-11-17 [2] RSPM (R 4.1.0) ## rlang 0.4.11 2021-04-30 [2] RSPM (R 4.1.0) ## rmarkdown 2.9 2021-06-15 [1] RSPM (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [2] RSPM (R 4.1.0) ## sessioninfo * 1.1.1 2018-11-05 [2] RSPM (R 4.1.0) ## stringi 1.6.2 2021-05-17 [2] RSPM (R 4.1.0) ## stringr 1.4.0 2019-02-10 [2] RSPM (R 4.1.0) ## withr 2.4.2 2021-04-18 [2] RSPM (R 4.1.0) ## xfun 0.24 2021-06-15 [2] RSPM (R 4.1.0) ## yaml 2.2.1 2020-02-01 [2] RSPM (R 4.1.0) ## ## [1] /usr/local/lib/R/host-site-library ## [2] /usr/local/lib/R/site-library ## [3] /usr/local/lib/R/library This book was last updated on 2021-07-09 16:08:22. "],["quick-start.html", "1 Quick Start 1.1 Setup 1.2 Configuration", " 1 Quick Start A brief guide to setting up SPEAQeasy. A more detailed and thorough guide is here. 1.1 Setup Clone the SPEAQeasy repository with git clone git@github.com:LieberInstitute/SPEAQeasy.git Change directory into the repository with cd SPEAQeasy Users of the JHPCE cluster should run bash install_software.sh \"jhpce\". For all other users with access to docker, run bash install_software.sh \"docker\". This will install nextflow and set up some test files. Otherwise, you can install everything required locally with bash install_software.sh \"local\". Note: JHPCE users must also make an edit to their ~/.bashrc files, described here 1.2 Configuration 1.2.1 Your “main” script The script you will use to run the pipeline depends on the system (“executor”) you wish to run the pipeline on. Executor Script SGE cluster run_pipeline_sge.sh SLURM cluster run_pipeline_slurm.sh local machine run_pipeline_local.sh The JHPCE cluster run_pipeline_jhpce.sh Options included in the main script should be modified as appropriate for the experiment. On SLURM and SGE clusters (including JHPCE), the main script should be submitted as a job (i.e. using sbatch or qsub). On local machines, the pipeline can be run interactively (i.e. bash run_pipeline_local.sh). Note: Docker users must make one additional edit to their main script: prepend “docker_” to the profile flag. For example, for SLURM users, change this line to -profile docker_slurm as seen here: $ORIG_DIR/Software/nextflow main.nf \\ --sample &quot;single&quot; \\ --reference &quot;hg19&quot; \\ --strand &quot;unstranded&quot; \\ --small_test \\ --annotation &quot;$ORIG_DIR/Annotation&quot; \\ -with-report execution_reports/pipeline_report.html \\ -profile docker_slurm # was &quot;-profile slurm&quot;! 1.2.2 Your config file Your combination of “executor” (SLURM cluster, SGE cluster, or local) and software management method (use docker or not use docker) determine the name of your config. Find your file under SPEAQeasy/conf/. Executor Using docker? Config Filename SGE cluster Yes docker_sge.config SGE cluster No sge.config SLURM cluster Yes docker_slurm.config SLURM cluster No slurm.config local machine Yes docker_local.config local machine No local.config The JHPCE cluster No jhpce.config As an example, suppose you have access to a SLURM cluster, and cannot use docker. Your config file is then SPEAQeasy/conf/slurm.config. "],["setup-details.html", "2 Setup Details 2.1 Requirements 2.2 Installation 2.3 Run the Pipeline 2.4 Sharing the pipeline with many users", " 2 Setup Details 2.1 Requirements SPEAQeasy requires that the following be installed: Java 8 or later Python 3 (tested with 3.7.3), with pip If java is not installed, you can install it on linux with apt install default-jre, or with a different package manager you prefer. Python 3 and pip (automatically installed with typical installations of python) are required as well. These installations are typically done by an administrator (they require root access/ use of “sudo”). SPEAQeasy has been tested on Linux, but it designed to run on any of a number of POSIX-compliant systems, including MacOS and FreeBSD. 2.2 Installation SPEAQeasy makes use of a number of different additional software tools. The user is provided two options to automatically manage these dependencies. Docker: The recommended option is to manage software with docker, if it is available. From within the repository, perform the one-time setup by running bash install_software.sh \"docker\". This installs nextflow and sets up some test files. When running SPEAQeasy, the required docker images are automatically pulled if not already present, and components of the pipeline run within the associated containers. A full list of the images that are used is here. Local install: The alternative is to locally install all dependencies. This option is only officially supported on Linux; it is currently experimental and recommended against on other platforms. Installation is done by running bash install_software.sh \"local\" from within the repository. This installs nextflow, several bioinformatics tools, R and packages, and sets up some test files. A full list of software used is here. The script install_software.sh builds each software tool from source, and hence relies on some common utilities which are often pre-installed in many unix-like systems: A C/C++ compiler, such as GCC or Clang The GNU make utility The makeinfo utility git, for downloading some software from their GitHub repositories The unzip utility Note: users at the JHPCE cluster do not need to worry about managing software via the above methods (required software is automatically available through modules). Simply run bash install_software.sh \"jhpce\" to install any missing R packages and set up some test files. Next, make sure you have the following lines added to your ~/.bashrc file: if [[ $HOSTNAME == compute-* ]]; then module use /jhpce/shared/jhpce/modulefiles/libd fi 2.2.1 Troubleshooting Some users may encounter errors during the installation process, particularly when installing software locally. We provide a list below of the most common installation-related issues. Please note that “local” installation is only officially supported on Linux, and Mac users should install in “docker” mode! Below solutions for Mac OS are experimental, and not yet complete. SPEAQeasy has been tested on: CentOS 7 (Linux) Ubuntu 20.04.2 LTS (Linux) 2.2.1.1 Required utilities are missing This is particularly common issue for MacOS users, or Linux users trying to get SPEAQeasy running on a local machine (like a laptop). In either case, we will assume the user has root privileges for the solutions suggested below. install_software.sh: line 99: wget: command not found Mac OS users in particular may be missing the utility wget (and others), required for the SPEAQeasy installation. A solution on Mac is to install brew, and install the required utilities through brew: # Install brew, if you don&#39;t already have it /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; # Install most required utilities brew install wget autoconf automake make gcc zlib bzip2 xz pcre openssl texinfo llvm libomp # Install openJDK8 (Java) brew install --cask homebrew/cask-versions/adoptopenjdk8 It is also recommended that Linux users install some basic dependencies if local installation fails for any reason. # On Debian or Ubuntu: sudo apt install autoconf automake make gcc zlib1g-dev libbz2-dev liblzma-dev libpcre3-dev libcurl4-openssl-dev texinfo texlive-base default-jre default-jdk # On RedHat or CentOS: sudo yum install autoconf automake make gcc zlib-devel bzip2 bzip2-devel xz-devel pcre-devel curl-devel texi2html texinfo java-1.8.0-openjdk java-1.8.0-openjdk-devel 2.2.1.2 Docker permissions issues Users managing dependencies with docker might encounter error messages if docker is not properly configured: docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See &#39;docker run --help&#39;. On a computing cluster, a system administrator is responsible for correctly configuring docker so that such errors do not occur. We provide a brief guide here for users who want to set up docker on a local machine. 2.3 Run the Pipeline The “main” script used to run the pipeline depends on the environment you will run it on. 2.3.1 Run in a SLURM environment/ cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/slurm.config, if you have installed software dependencies locally, or conf/docker_slurm.config if you will use docker. Modify the main script and run: the main script is run_pipeline_slurm.sh. Submit as a job to your cluster with sbatch run_pipeline_slurm.sh. If you are using docker, make sure to change the line -profile slurm to profile docker_slurm. See the full list of command-line options for other details about modifying the script for your use-case. See here for Nextflow’s documentation regarding SLURM environments. 2.3.2 Run on a Sun Grid Engines (SGE) cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/sge.config, if you have installed software dependencies locally, or conf/docker_sge.config if you will use docker. Modify the main script and run: the main script is run_pipeline_sge.sh. Submit as a job to your cluster with qsub run_pipeline_sge.sh. If you are using docker, make sure to change the line -profile sge to profile docker_sge. See the full list of command-line options for other details about modifying the script for your use-case. See here for additional information on nextflow for SGE environments. 2.3.3 Run locally (Optional) Adjust configuration: hardware resource usage and other configurables are located in conf/local.config, if you have installed software dependencies locally, or conf/docker_local.config if you will use docker. Note that defaults assume access to 8 CPUs and 16GB of RAM. Modify the main script and run: the main script is run_pipeline_local.sh. If you are using docker, make sure to change the line -profile local to profile docker_local. After configuring options for your use-case (See the full list of command-line options), simply run on the command-line with bash run_pipeline_local.sh. 2.3.4 Run on the JHPCE cluster (Optional) Adjust configuration: default configuration with thoroughly testing hardware resource specification is described within conf/jhpce.config. Other settings, such as annotation release/version, can also be tweaked via this file. Modify the main script and run: the “main” script is run_pipeline_jhpce_qsub.sh. The pipeline run is submitted as a job to the cluster by executing qsub run_pipeline_jhpce.sh. See the full list of command-line options for other details about modifying the script for your use-case. 2.3.5 Example main script Below is a full example of a typical main script, modified from the run_pipeline_jhpce.sh script. At the top are some cluster-specific options, recognized by SGE, the grid scheduler at the JHPCE cluster. These are optional, and you may consider adding appropriate options similarly, if you plan to use SPEAQeasy on a computing cluster. After the main call, nextflow /dcl01/lieber/ajaffe/Nick/SPEAQeasy/main.nf, each command option can be described line by line: --sample \"paired\": input samples are paired-end --reference \"mm10\": these are mouse samples, to be aligned to the mm10 genome --strand \"reverse\": the user expects the samples to be reverse-stranded, which SPEAQeasy will verify --ercc: the samples have ERCC spike-ins, which the pipeline should quantify as a QC measure. --trim_mode \"skip\": trimming is not to be performed on any samples --experiment \"mouse_brain\": the main pipeline outputs should be labelled with the experiment name “mouse_brain” --input \"/users/neagles/RNA_input\": /users/neagles/RNA_input is a directory that contains the samples.manifest file, describing the samples. -profile jhpce: configuration of hardware resource usage, and more detailed pipeline settings, is described at conf/jhpce.config, since this is a run using the JHPCE cluster -w \"/scratch/nextflow_runs\": this is a nextflow-specific command option (note the single dash), telling SPEAQeasy that temporary files for the pipeline run can be placed under /scratch/nextflow_runs --output \"/users/neagles/RNA_output\": SPEAQeasy output files should be placed under /users/neagles/RNA_output #!/bin/bash #$ -l bluejay,mem_free=40G,h_vmem=40G,h_fsize=150G #$ -o ./run_brain_subset.log #$ -e ./run_brain_subset.log #$ -cwd module use /jhpce/shared/jhpce/modulefiles/libd module load nextflow export _JAVA_OPTIONS=&quot;-Xms8g -Xmx10g&quot; nextflow /dcl01/lieber/ajaffe/Nick/SPEAQeasy/main.nf \\ --sample &quot;paired&quot; \\ --reference &quot;mm10&quot; \\ --strand &quot;reverse&quot; \\ --ercc \\ --trim_mode &quot;skip&quot; \\ --experiment &quot;mouse_brain&quot; \\ --input &quot;/users/neagles/RNA_input&quot; \\ -profile jhpce \\ -w &quot;/scratch/nextflow_runs&quot; \\ --output &quot;/users/neagles/RNA_output&quot; # Produces a report for each sample tracing the pipeline steps # performed (can be helpful for debugging). # # Note that the reports are generated from the output log produced in the above # section, and so if you rename the log, you must also pass replace the filename # in the bash call below. echo &quot;Generating per-sample logs for debugging...&quot; bash /dcl01/lieber/ajaffe/Nick/SPEAQeasy/scripts/generate_logs.sh $PWD/run_brain_subset.log 2.3.6 Advanced info regarding installation If you are installing software to run the pipeline locally, all dependencies are installed into [repo directory]/Software/, and [repo directory]/conf/command_paths_long.config is configured to show nextflow the default installation locations of each software tool. Thus, this config file can be tweaked to manually point to different paths, if need be (though this shouldn’t be necessary). Nextflow supports the use of Lmod modules to conveniently point the pipeline to the bioinformatics software it needs. If you neither wish to use docker nor wish to install the many dependencies locally– and already have Lmod modules on your cluster– this is another option. In the appropriate config file (as determined in step 3 in the section you choose below), you can include a module specification line in the associated process (such as module = 'hisat2/2.2.1' for buildHISATindex) as configured in conf/jhpce.config. In most cases this will be more work to fully configure, and so running the pipeline with docker or locally installing software is generally recommended instead. See nextflow modules for some more information. 2.4 Sharing the pipeline with many users A single installation of SPEAQeasy can be shared among potentially many users. New users can simply copy the appropriate “main” script (determined above) to a different desired directory, and modify the contents as appropriate for the particular experiment. Similarly, a single user can copy the “main” script and modify the copy whenever there is a new experiment/ set of samples to process, reusing a single installation of SPEAQeasy arbitrarily many times. Note It is recommended to use a unique working directory with the -w option for each experiment. This ensures: SPEAQeasy resumes from the correct point, if ever stopped while multiple users are running the pipeline Deleting the work directory (which can take a large amount of disk space) does not affect SPEAQeasy execution for other users or other experiments New users who wish to include the --coverage option must also install RSeQC personally: python3 -m pip install --user RSeQC==3.0.1 2.4.1 Customizing execution for each user By default, all users will share the same configuration. This likely suffices for many use cases, but alternatively new configuration files can be created. Below we will walk through an example where a new user of a SLURM-based cluster wishes to use an existing SPEAQeasy installation, but wants a personal configuration file to specify different annotation settings. Copy the existing configuration to a new file # Verify we are in the SPEAQeasy repository pwd # Create the new configuration file cp conf/slurm.config conf/my_new.config Modify the new file as desired Below we will change the GENCODE release to the older release 25, for human, via the gencode_version_human variable. executor = &#39;slurm&#39; params { gencode_version_human = &quot;25&quot; // originally was &quot;32&quot;! gencode_version_mouse = &quot;M25&quot; ensembl_version_rat = &quot;98&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) See configuration for details on customizing SPEAQeasy settings. Add the new file as a “profile” This involves adding some code to nextflow.config, as shown below. profiles { // Here we&#39;ve named the new profile &quot;my_new_config&quot;, and pointed it to the // file &quot;conf/my_new.config&quot;. Note that docker users should have the 2nd line // &quot;includeConfig &#39;conf/command_paths_short.config&#39;&quot; instead! my_new_config { includeConfig &#39;conf/my_new.config&#39; includeConfig &#39;conf/command_paths_long.config&#39; } // This configuration had already existed local { includeConfig &#39;conf/local.config&#39; includeConfig &#39;conf/command_paths_long.config&#39; } Reference the new profile in the “main” script Recall that new users should copy the “main” script and modify the copy as appropriate. In this case, we open a copy of the original run_pipeline_slurm.sh: # At the nextflow command, we change the &#39;-profile&#39; argument at the bottom $ORIG_DIR/Software/nextflow main.nf \\ --sample &quot;single&quot; \\ --reference &quot;hg19&quot; \\ --strand &quot;unstranded&quot; \\ --small_test \\ --annotation &quot;$ORIG_DIR/Annotation&quot; \\ -with-report execution_reports/pipeline_report.html \\ -with-dag execution_DAGs/pipeline_DAG.html \\ -profile my_new_config # this was changed from &quot;-profile slurm&quot;! "],["command-opts.html", "3 All Command Options 3.1 SPEAQeasy Options 3.2 Nextflow Options", " 3 All Command Options 3.1 SPEAQeasy Options 3.1.1 Mandatory Parameters --sample “single” or “paired”: the orientation of your reads --strand “unstranded”, “forward”, or “reverse”: the strandness of your reads. Since strandness is inferred by sample in the pipeline, this option informs the pipeline to generate appropriate warnings if unexpected strandness is inferred. --reference “hg38”, “hg19”, “mm10”, or “rn6”: the reference genome to which reads are aligned 3.1.2 Optional Parameters --annotation The path to the directory containing pipeline annotations. Defaults to “./Annotation” (relative to the repository). If annotations are not found here, the pipeline includes a step to build them. --coverage Include this flag to produce coverage bigWigs and compute expressed genomic regions. These steps are a useful precursor for analyses involving finding differentially expressed regions (DERs). Default: false --custom_anno [label] Include this flag to indicate that the directory specified with --annotation [dir] includes user-provided annotation files to use instead of the default files. See the “Using custom annotation” section for more details. --ercc Include this flag to enable ERCC quantification with Kallisto (or Salmon, if --use_salmon is provided). This is appropriate for experiments where samples have ERCC spike-ins. --experiment Name of the experiment being run (ex: “alzheimer”). Defaults to “Jlab_experiment” --force_strand Include this flag to continue pipeline execution with a warning, when user-provided strand contrasts with inferred strandness in any sample. Default: false (halt pipeline execution with an error message if any sample appears to be a different strandness than stated by the user) --fullCov Include this flag to perform full coverage analysis. --help Provides an explanation about each of these command options, and exits. --input The path to the directory with the “samples.manifest” file. Defaults to “./input” (relative to the repository) --keep_unpaired include this flag to keep unpaired reads output from trimming paired-end samples, for use in alignment. Default: false, as this can cause issues in downstream tools like FeatureCounts. --output The path to the directory to store pipeline output files/ objects. Defaults to “./results” (relative to the repository) --prefix An additional label (in conjunction with “experiment”) to use for the pipeline run- affecting result file names. --small_test Uses sample files located in the test folder as input. Overrides the “–input” option. --trim_mode Determines the conditions under which trimming occurs: “skip”: do not perform trimming on samples “adaptive”: [default] perform trimming on samples that have failed the FastQC “Adapter content” metric “force”: perform trimming on all samples --unalign Include this flag to save discordant reads (when using HISAT2) or unmapped reads (when using STAR) after the alignment step (false/ not included by default) --use_salmon Include this flag to quantify transcripts with Salmon rather than the default of Kallisto. --use_star Include this flag to use STAR during alignment, instead of the default of HISAT2. 3.2 Nextflow Options The nextflow command itself provides many additional options you may add to your “main” script. A few of the most commonly applicable ones are documented below. For a full list, type [path to nextflow] run -h- the full list does not appear to be documented at nextflow’s website. -w [path] Path to the directory where nextflow will place temporary files. This directory can fill up very quickly, especially for large experiments, and so it can be useful to set this to a scratch directory or filesystem with plenty of storage capacity. -resume Include this flag if pipeline execution halts with an error for any reason, and you wish to continue where you left off from last run. Otherwise, by default, nextflow will restart execution from the beginning. -with-report [filename] Include this to produce an html report with execution details (such as memory usage, completion details, and much more) N [email address] Sends email to the specified address to notify the user regarding pipeline completion. Note that nextflow relies on the sendmail tool for this functionality- therefore sendmail must be available for this option to work. "],["pipeline-overview.html", "4 Pipeline Overview 4.1 Preparation Steps 4.2 Main Workflow Steps", " 4 Pipeline Overview Diagram representing the “conceptual” workflow traversed by SPEAQeasy. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated below. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, preparing a particular set of annotation files occurs once and uses a cache for further runs. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. 4.1 Preparation Steps The following processes in the pipeline are done only once for a given configuration, and are skipped on all subsequent runs: 4.1.1 Downloading required annotation files PullAssemblyFasta: when using default annotation, this process pulls the genome fasta from GENCODE or Ensembl and saves to the directory specified by --annotation. This is the file against which FASTQ reads are aligned. PullGtf: similarly to PullAssemblyFasta, this process pulls the “.gtf” transcript annotation file. PullTranscriptFasta: similar to the above two processes, this one pulls the fasta of transcript sequences (against which FASTQ reads are pseudoaligned). 4.1.2 Preparing annotation files for direct use BuildAnnotationObjects: a number of files are produced for internal use by SPEAQeasy. These include a text file of sequence ranges and a few R data files with annotation information (regarding genes, junctions, and exons). BuildHISATIndex: the HISAT2 aligner requires an indexed genome- this process builds that file for the given genome fasta. BuildKallistoIndex: similarly to HISAT2, the transcript quantification tool Kallisto requires that the transcripts be indexed. This process builds the required index. BuildSalmonIndex: when Salmon is used as the pseudoaligner (with the command option --use_salmon), an index like that for Kallisto is built in this process. 4.2 Main Workflow Steps QualityUntrimmed: FASTQ files are ran through FastQC as a preliminary quality control measure. By default, the presence of “adapter content” determined in this process decides whether a given sample is trimmed in the next step. See the --trim_mode command option for modifying this default behavior. Trimming: See “QualityUntrimmed” above- FASTQ inputs are trimmed either based on adapter content from FastQC, or regardless of this measure. QualityTrimmed: a post-trimming assessment of quality metrics (again with FastQC) for each sample. InferStrandness: determines the “strandness” of each sample (“forward”, “reverse”, or “unstranded”/mixed). This is done via pseudoalignment to the transcriptome with Kallisto- for a subset of reads in a given sample, pseudoalignment is attempted assuming each of the three strandness possibilities. The number of successfully aligned reads in each case is used to determine a final “observed strandness”. By default an error is thrown for any disagreement between “observed strandness” and user-provided --strand. See the --force_strand command option and the configuration variable num_reads_infer_strand for tuning the behavior of this process. CompleteManifest: this process is mostly for internal use by the pipeline: an additional “observed strandness” column is added to the user-provided samples.manifest to associate each sample with a strandness value (required for many processes, such as alignment). This file is also saved to the pipeline outputs, allowing the user to see in one location the strandness determination for each sample. SingleEndHISAT and PairedEndHISAT: though single-end and paired-end alignment are managed by separate processes internally, both perform alignment to the genome with HISAT2, outputting mapped reads in BAM format. BamSort: coordinate-sort and index the BAM output from HISAT2 or STAR. PrimaryAlignments: subsets the alignment BAM to only “primary alignments”, as determined by the samtools bit-wise flag 0x100. Indexes the result file. Junctions: extracts and quantifies exon-exon junctions from the BAM of primary alignments. Coverage: an optional process (see the --coverage command option) to convert the alignment BAMs to .wig format. This is in preparation for the WigToBigWig process, and does not have outputs/results of its own (other than logs). WigToBigWig: the follow-up process to Coverage, producing BigWig files containing coverage information. MeanCoverage: combines coverage information for each strand, by taking a mean of bigwig coverage across all samples for each strand. Typically, one pipeline run will have just one strand (and will match what is provided by the --strand option), but if different samples have different observed strandness (see InferStrandness process), and the --force_strand option is provided, samples are separated by strand in this process. ExpressedRegions: the “final” process in the path from alignment SAM to expression data. This process utilizes the R package derfinder to determine expressed genomic regions, and a number of plots and RData objects are produced to summarize and visualize coverage and expression-related information. FeatureCounts: this process produces counts of genes and exons from the BAM produced by SamtoBam. TXQuantKallisto or TXQuantSalmon: these steps perform pseudoalignment of FASTQ reads to the transcriptome. We have found that Kallisto performs this job with superior speed for equal or better quality results, and thus it is the default tool used. The process TXQuantSalmon is run instead when the option --use_salmon is provided. ERCC: this is an optional process which is run when the option --ercc is specified. This performs pseudoalignment with Kallisto to a special index as defined by the External RNA Controls Consortium. This process is intended to be run only for samples which contain ERCC spike-ins, as a quality-control measure. CountObjects: this process collects gene and exon counts from FeatureCounts, exon-exon junction counts from Junctions, QC metrics from QualityUntrimmed, transcript pseudoalignment results from TXQuantKallisto or TXQuantSalmon, and results from ERCC if applicable. The result are analysis-ready R data files and RangedSummarizedExperiment R objects, combining key information gathered throughout the pipeline run. VariantCalls: this process is performed for human references only (i.e. “hg38” or “hg19” values for --reference) VariantsMerge: this process combines the per-sample vcf files from VariantCalls into a single vcf for the experiment. CoverageObjects: this process takes the coverage bigwig files from WigToBigWig and calculates “full coverage”- namely, the fullCoverage function is called, from the Bioconductor package derfinder. This process is not run by default, but can be enabled via the option --fullCov. "],["annotation.html", "5 Annotation 5.1 Default Annotation 5.2 Custom Annotation", " 5 Annotation SPEAQeasy can be run with hg38, hg19, mm10, or rn6 references. The pipeline has a default and automated process for pulling and building annotation-related files, but the user can opt to provide their own annotation as an alternative. Both of these options are documented below. In general, the pipeline uses three types of files. Example files below are the ones used with default configuration when hg38 reference is selected. A genome assembly fasta: the reference genome to align reads to, like the file here (but unzipped) Gene annotation gtf: containing transcript data, like the file here (but unzipped) A transcripts fasta: with the actual transcript sequences, such as the file here (but unzipped) 5.1 Default Annotation SPEAQeasy uses annotation files provided by GENCODE where possible- which is for references hg38, hg19, and mm10. For rn6, files are pulled directly from Ensembl. 5.1.1 Choosing a release With genome and transcript annotation constantly being updated, the user may want to use a particular GENCODE release or Ensembl version. For each species, there is a corresponding configuration variable you may set to control the versions used. The variables gencode_version_human and gencode_version_mouse refer to the GENCODE release number. Similarly, the variable ensembl_version_rat specifies the Ensembl version for “rn6” reference. /* * * A config file for execution in a SLURM environment * */ executor = &#39;slurm&#39; params { gencode_version_human = &quot;32&quot; gencode_version_mouse = &quot;M25&quot; ensembl_version_rat = &quot;98&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) 5.1.2 Choosing a “build” Depending on the analysis you are doing, you may wish to only consider the reference chromosomes (for humans, the 25 sequences chr1 through chrM) for alignment and transcript quantification. SPEAQeasy provides the option to choose from two annotation “builds” for a given release and reference, called “main” and “primary” (following the naming convention from GENCODE databases). The “main” build consists of only the canonical “reference” sequences for each species The “primary” build consists of the canonical “reference” sequences and additional scaffolds, as a genome primary assembly fasta from GENCODE would contain. See the variable annotation_build in your configuration file for making this selection for your pipeline run. 5.1.3 Additional Annotation files A .bed file containing common SNV (single nucleotide variation) sites, at which variant calling is performed (for human and mouse). Variant calling is not currently supported for rat or mouse (rn6 or mm10, respectively). An ERCC index: this is a file generated by Kallisto to prepare for quantifying ERCC spike-ins. The index is produced from the FASTA of ERCC transcript sequences, as in the file [SPEAQeasy repo]/Annotation/ERCC/ERCC92.fa. 5.2 Custom Annotation You may wish to provide specific reference files in place of the automatically managed files described in the above section. In this case, you must supply the following files in the directory specified in the command-line option --annotation [dir]: A genome assembly fasta (the reference genome to align reads to), such as the file here. Make sure the file has the string “assembly” in the filename, to specify to the pipeline that it is the genome reference fasta. Gene annotation gtf, such as the file here- but not gzipped. This file can have any name, so long as it ends in “.gtf”. A transcripts fasta, such as the file here- but not gzipped. Make sure to include “transcripts” anywhere in the filename (provided the file ends in “.fa”) to differentiate this file from the reference genome. 5.2.1 Optional files to include depending on your use-case An ERCC index (this is a file specific to Kallisto needed for ERCC quantification, which is an optional component of the pipeline). You can find the index used by default at [repository directory]/Annotation/ERCC/ERCC92.idx. This file must end in “.idx”. A list of SNV sites at which to call variants (in .bed format). Variant calling is by default only enabled for human reference. You can find the .bed files used by default for “hg38” and “hg19” at [repository directory]/Annotation/Genotyping/common_missense_SNVs_hg*.bed. This file can have any name provided it has the “.bed” extension. You must also add the --custom_anno [label] argument to your run_pipeline_X.sh script, to specify you are using custom annotation files. The “label” is a string you want to include in filenames generated from the annotation files you provided. This is intended to allow the use of potentially many different custom annotations, assigned a unique and informative name you choose each time. This can be anything except an empty string (which internally signifies not to use custom annotation). "],["manifest.html", "6 Manifest and Inputs 6.1 What the Manifest Should Look Like 6.2 Creating a manifest file", " 6 Manifest and Inputs Inputs to SPEAQeasy are specified by a single file named samples.manifest. The samples.manifest file associates each FASTQ file with a path and ID, and allows the pipeline to automatically merge files if necessary. 6.1 What the Manifest Should Look Like Each line in samples.manifest should have the following format: For a set of unpaired reads &lt;PATH TO FASTQ FILE&gt;(tab)&lt;optional MD5&gt;(tab)&lt;sample label/id&gt; For paired-end sets of reads &lt;PATH TO FASTQ 1&gt;(tab)&lt;optional MD5 1&gt;(tab)&lt;PATH TO FASTQ 2&gt;(tab)&lt;optional MD5 2&gt;(tab)&lt;sample label/id&gt; A line of paired-end reads could look like this: RNA_sample1_read1.fastq 0 RNA_sample1_read2.fastq 0 sample1 The MD5(s) on each line are for compatibility with a conventional samples.manifest structure, and are not explicitly checked in the pipeline (you may simply use 0s as in the above example). Paths must be long/full. If you have a single sample split across multiple files, you can signal for the pipeline to merge these files by repeating the sample label/id on each line of files to merge. A samples.manifest file cannot include both single-end and paired-end reads; separate pipeline runs should be performed for each of these read types. This is an example of a samples.manifest file for some paired-end samples. Note how the first sample “dm3” is split across more than one pair of files, and is to be merged: /scratch/dm3_file1_1.fastq 0 /scratch/dm3_file1_2.fastq 0 dm3 /scratch/dm3_file2_1.fastq 0 /scratch/dm3_file2_2.fastq 0 dm3 /scratch/sample_01_1.fastq.gz 0 /scratch/sample_01_2.fastq.gz 0 sample_01 /scratch/sample_02_1.fastq.gz 0 /scratch/sample_02_2.fastq.gz 0 sample_02 6.1.1 More details regarding inputs Input FASTQ files can have the following file extensions: .fastq, .fq, .fastq.gz, .fq.gz FASTQ files must not contain “.” characters before the typical extension (e.g. sample.1.fastq), since some internal functions rely on splitting file names by “.”. 6.2 Creating a manifest file In a common scenario, you may have a large number of FASTQ files in a single directory, for a given experiment. How can the samples.manifest file be constructed in this case? While the method you use is a matter of preference, we find it straightforward to write a small R script to generate the manifest. Suppose we have 3 paired-end samples, consisting of a total of 6 FASTQ files: /data/fastq/SAMPLE1_L001_R1_001.fastq.gz /data/fastq/SAMPLE1_L001_R2_001.fastq.gz /data/fastq/SAMPLE2_L002_R1_001.fastq.gz /data/fastq/SAMPLE2_L002_R2_001.fastq.gz /data/fastq/SAMPLE3_L003_R1_001.fastq.gz /data/fastq/SAMPLE3_L003_R2_001.fastq.gz The following script can generate the manifest appropriate for this experiment: # If needed, install the &#39;jaffelab&#39; GitHub-based package, which includes a # useful function for string manipulation remotes::install_github(&quot;LieberInstitute/jaffelab&quot;) library(&quot;jaffelab&quot;) fastq_dir &lt;- &quot;/data/fastq&quot; # We can take advantage of the uniform file naming convention to get the paths # of each mate in the pair, for every sample. Here we use a somewhat # complicated regular expression to match file names (to be sure we are # matching precisely the files we think we&#39;re matching), but this can be kept # simple if preferred. r1 &lt;- list.files(fastq_dir, &quot;.*_L00._R1_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) r2 &lt;- list.files(fastq_dir, &quot;.*_L00._R2_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) # We can form a unique ID for each sample by taking the portion of the path to # the first read preceding the lane and mate identifiers. The function &#39;ss&#39; is # a vectorized form of &#39;strsplit&#39;, handy for this task ids &lt;- ss(basename(r1), &quot;_L00&quot;) # Sanity check: there should be the same number of first reads as second reads stopifnot(length(R1) == length(R2)) # Prepare the existing sample information into the format expected by # SPEAQeasy (for now, as a character vector where each element will be a line # in &#39;samples.manifest&#39;). We will simply use zeros for the optional MD5 sums. manifest &lt;- paste(r1, 0, r2, 0, ids, sep = &quot;\\t&quot;) # Write the manifest to a file (in this case, in the current working # directory) writeLines(manifest, con = &quot;samples.manifest&quot;) "],["outputs.html", "7 Pipeline Outputs 7.1 Main Outputs 7.2 Intermediary Outputs", " 7 Pipeline Outputs 7.1 Main Outputs Main Outputs Most importantly, SPEAQeasy generates RangedSummarizedExperiment objects storing counts at several features (genes, exons, and exon-exon junctions). A number of Bioconductor packages can be trivially utilized to perform desired differential expression analyses from counts in the widespread SummarizedExperiment format, and we provide a vignette demonstrating an example analysis. For human samples, variants are called at a list of common missense single nucleotide variants (SNVs), and SPEAQeasy ultimately a single VCF file to store genotype calls at these sites for all samples in the experiment. We provide a guide walking through how this genotype data can be used to resolve identity issues that arise during sequencing, salvaging samples which otherwise might be dropped from further analysis. Finally, expressed regions data is optionally generated (with the --coverage option). The RData files provide a starting point for finding differentially expressed regions (DERs), for analyses involving this end goal. 7.1.1 Quality Metrics One of the major pipeline outputs is a comma-separated values (CSV) file where fields (columns) are different quality metrics, and each line (row) is associated with one sample. A list of the exact field names and their descriptions is given below. Metric name Description SAMPLE_ID The name of the sample, as specified in the last column of samples.manifest ERCCsumLogErr If applicable, a summary statistic quantifying overall difference of expected and actual ERCC concentrations for one sample trimmed A boolean value (“TRUE” or “FALSE”), indicating whether the given sample underwent trimming numReads The number of reads present in any FASTQ files associated with the sample (after any trimming) numMapped The number of reads which successfully mapped to the reference genome during alignment numUnmapped The number of reads which did not successfully map to the reference genome during alignment overallMapRate The decimal fraction of reads which successfully mapped to the reference genome (i.e. numMapped / numReads) concordMapRate The decimal fraction of reads which aligned concordantly to the reference genome totalMapped The number of reads which successfully mapped to the canonical sequences in the reference genome (excluding mitochondrial chromosomes) mitoMapped The number of reads which successfully mapped to the mitochondrial chromosome mitoRate The decimal fraction of reads which mapped to the mitochondrial chromosome, of those which map at all (i.e. mitoMapped / (totalMapped + mitoMapped)) totalAssignedGene The decimal fraction of reads assigned unambiguously to a gene, with featureCounts (Liao et al. 2014), of those in total rRNA_rate The decimal fraction of reads assigned to a gene whose type is ‘rRNA’, of those assigned to any gene 7.2 Intermediary Outputs SPEAQeasy generates a number of files along the process before producing the main outputs of interest. Each of these “intermediary” files is described below. Count Objects count_objects/ ercc_spikein_check_mix1.pdf: A plot comparing the Mix1 expected concentration against the observed counts from Kallisto. rawCounts_[experiment_name]_n[num_samples].rda: An R data file containing a number of matrices/ data frames containing raw gene, exon, exon-exon junction, and transcript counts. A data frame of quality metrics is also included. read_and_alignment_metrics_[experiment_name].csv: A comma-separated values file of quality metrics. rpkm_counts_[experiment_name]_n[num_samples].rda: The same data as in rawCounts_[experiment_name]_n[num_samples].rda, but normalized as reads-per-kilobase-million (RPKM). Raw Counts counts/ [sample_name]_[annotation_version]_Exons.counts`: Exon counts for each sample individually, as reported by featureCounts. [sample_name]_[annotation_version]_Exons.counts.summary: A summary of key information generated from quantifying exons on a particular sample with featureCounts. [sample_name]_[annotation_version]_Gene.counts: Gene counts for each sample individually, as reported by featureCounts. [sample_name]_[annotation_version]_Gene.counts.summary: A summary of key information generated from quantifying genes on a particular sample with featureCounts. junction/[sample_name]_junctions_primaryOnly_regtools.bed: A file in BED format describing junctions determined by regtools run on primary alignments for a given sample. junction/[sample_name]_junctions_primaryOnly_regtools.count: A text file describing the junction ranges and providing raw counts of hits for each junction, as output by regtools. junction/primary_alignments/[sample_name].bam and junction/primary_alignments/[sample_name].bam.bai: The subset of primary alignments for a given sample, in BAM format, along with an index for each BAM. ERCC Raw Quantification ERCC/ ERCC/[sample_name]/abundance.tsv: The output from running Kallisto against the 92 ERCC RNA Spike Mix sequences for each sample. Coverage wigs and bigWigs coverage/ [sample_name].[strand].wig: Wiggle files containing coverage information for a given sample and strand. mean/mean.forward.bw and mean/mean.reverse.bw: BigWig files containing coverage information averaged across all samples in the experiment, and split by strand. Expressed Regions Data expressed_regions/ region_cuts_raw_[strand].Rdata: An R object called region_cuts_raw that is a list with one element per chromosome, then a nested element per cutoff used for identifying expressed regions using derfinder::findRegions(). region_cuts_[strand].Rdata: An R object called region_cuts that is a list with one element per cutoff that contains a GenomicRanges::GRanges() object with the expressed regions across all chromosomes. region_info_[strand].Rdata: An R object called regInfo that is a data.frame with the columns: cutoff, n, mean, and sd. This table summarizes the number, mean width, sd of the width for the expressed regions identified at each cutoff. region_info_[strand].pdf: A PDF with a few exploratory plots made using regInfo that evaluate how the cutoff for identifying the expressed regions affects the number of ERs, their mean width, the sd of their width. These are the visualization plots recommended for choosing a cutoff as described on the derfinder manuscript at Figure S4 available here. FastQC Outputs fastQC/ [trim_status]/[file_name]/*`: Outputs from FastQC. Here trim_status indicates when FastQC was performed: Untrimmed is before trimming, and Trimmed is after. file_name contains the sample name, and if applicable, the mate number. Alignment BAMs and Summaries alignment/ [sample_name].bam: The main alignment output from Hisat2 or optionally STAR in BAM format. In either case, unmapped reads are not included (different from the default behavior for HISAT2!). [sample_name]_align_summary.txt: The text-based alignment summary from Hisat2, if applicable. Note that metrics from these files are aggregated for the experiment, and so users likely will not need to check or process the original files manually. [sample_name]_STAR_alignment.log: Statistics from STAR alignment (if applicable) for a single sample, renamed from Log.final.out. Note that metrics from these files are aggregated for the experiment, and so users likely will not need to check or process the original files manually. bam_sort/[sample_name]_sorted.bamandbam_sort/[sample_name]_sorted.bam.bai`: Coordinate-sorted alignments and their corresponding indices. [sample_name]_unmapped_mate*.fastq: If using STAR with paired-end reads and the --unalign option, these two files (mates 1 and 2) are produced, including unmapped reads (this includes “discordant” reads). [sample_name]_discordant.fastq: If using HISAT2 (this is the default) and the --unalign option, this file is produced and includes discordant mappings. Outputs from Strand Inference infer_strandness/ [sample_name]_strandness_pattern.txt: A text file containing the inferred strandness pattern for each sample. These files are primarily for internal use by the pipeline, and it is recommended to check the output file samples_complete.manifest to quickly view strandness patterns for all samples. samples_complete.manifest: A version of the input samples.manifest, with an additional column listing the inferred strandness pattern for each sample Transcript Quantification by Sample kallisto_tx/ [sample_name]/abundance.h5: Abundance estimates, bootstrap estimates, run metadata, and transcript length from Kallisto, saved into an HDF5 binary file. [sample_name]/[sample_name]_abundance.tsv: Plain-text abundance estimates from Kallisto across the reference transcriptome. [sample_name]/run_info.json: Kallisto run metadata in javascript object notation. SPEAQeasy Logs by Sample logs/ [sample_name]_process_trace.log: A SPEAQeasy-generated log tracing the processes and associated commands run for each sample. This is intended to help users quickly determine the source of any errors during pipeline execution (text). Trimmed FASTQ Files trimming/ [sample_name]_trimmed*.fastq: Trimmed FASTQ files, if applicable, from Trimmomatic "],["configuration.html", "8 Configuration 8.1 Specifying Options for your Cluster 8.2 SPEAQeasy Parameters", " 8 Configuration SPEAQeasy is designed to be highly customizable, yet need no configuration from a user wishing to rely on sensible default settings. Most configuration, including software settings, hardware resources such as memory and CPU use, and more, can be done in a single file determined here. Please note that the common, “major” options are specified in your main script, and not in the configuration file. The config is intended to hold the “finer details”, which are documented below. 8.1 Specifying Options for your Cluster In many cases, a user has access to a computing cluster which they intend to run SPEAQeasy on. If your cluster is SLURM or SGE-based, the pipeline is pre-configured with options you may be used to specifying (such as disk usage, time for a job to run, etc). However, these are straightforward to modify, should there be a need/desire. Common settings are described in detail below; however, a more comprehensive list of settings from nextflow can be found here. 8.1.1 Time The maximum allowed run time for a process, or step in the pipeline, can be specified. This may be necessary for users who are charged based on run time for their jobs. 8.1.1.1 Default for all processes The simplest change you may wish to make is to relax time constraints for all processes. The setting for this is here: executor { name = &#39;sge&#39; queueSize = 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;30 min&#39; } process { time = 10.hour // this can be adjusted as needed errorStrategy = { task.exitStatus == 140 ? &#39;retry&#39; : &#39;terminate&#39; } maxRetries = 1 While the syntax is not strict, some examples for properly specifying the option are time = '5m', time = '2h', and time = '1d' (minutes, hours, and days, respectively). 8.1.1.2 Specify per-process Time restrictions may also be specified for individual workflow steps. This can be done with the same syntax- suppose you wish to request 30 minutes for a given sample to be trimmed (note process names here). In the “process” section of your config, find the section labelled “withName: Trimming” as in the example here: withName: Trimming { cpus = 4 memory = 16.GB time = &#39;30m&#39; } 8.1.2 Cluster-specific options In some cases, you may find it simpler to directly specify options accepted by your cluster. For example, SGE users with a default limit on the maximum file size they may write might specify the following: withName: PairedEndHISAT { cpus = 4 memory = &#39;20.GB&#39; penv = &#39;local&#39; clusterOptions = &#39;-l h_fsize=500G&#39; // This is SGE-specific syntax } As with the time option, this can be specified per-process or for all processes. Any option recognized by your cluster may be used. 8.2 SPEAQeasy Parameters A number of variables in your config file exist to control choices about annotation to use, options passed to software tools, and more. These need not necessarily be changed, but allow more precise control over the pipeline if desired. Values for these variables may be changed in the “params” section of your config (near the top). A full descriptive list is provided below. 8.2.1 Annotation settings gencode_version_human: the GENCODE release to use for default (non-custom) annotation, when “hg38” or “hg19” references are used. A string, such as “32”. gencode_version_mouse: the GENCODE release to use for default (non-custom) annotation, when “mm10” reference is used. A string, such as “M23”. ensembl_version_rat: the Ensembl version to use for default (non-custom) annotation, when “rn6” reference is used. A string, such as “98”. anno_build: controls which sequences are used in analysis with default (non-custom) annotation. “main” indicates only canonical reference sequences; “primary” includes additional scaffolds. 8.2.2 Software settings Command-line parameters can be directly passed to most software tools used in the pipeline, through configuration. Note that SPEAQeasy internally passes certain command-line options to some software tools, in a way that can not be overriden in the configuration file. This is done for options which control parallelization, file naming, or other functions which must either be fixed or are configurable through other means (e.g. number of threads is implicitly controlled by the cpus variable in the QualityUntrimmed process, rather than directly via the -t argument to fastqc). Open your particular config file determined here for details of which options are fixed vs. which can be specified through configuration. A list of variables which control command-line arguments is as follows: bam2wig_args: a string of optional arguments to pass to bam2wig.py from RSeQC, when producing coverage wig files from BAM alignments. bcftools_args: a string of optional arguments to pass to bcftools call when calling variants (applicable for human samples only). fastqc_args: a string of optional arguments to pass to fastqc when performing quality checking before (and if applicable, after) trimming. feat_counts_gene_args and feat_counts_exon_args: strings of optional arguments passed to FeatureCounts when counting genes and exons, respectively. hisat2_args: a string of optional arguments to pass to hisat2-align. kallisto_len_mean and kallisto_len_sd: the mean and standard deviation for fragment length, required by kallisto quant when performing pseudoalignment. It is recommended you directly modify these variables rather than individually modifying kallisto_quant_single_args andkallisto_quant_ercc_single_args. These variables are also used when performing our pseudo-alignment-based approach to inferring strandness with single-end samples. kallisto_quant_single_args and kallisto_quant_paired_args: a string of arguments to pass to kallisto quant when performing pseudo-alignment to the reference transcriptome, for single-end and paired-end samples, respectively. kallisto_quant_ercc_single_args and kallisto_quant_ercc_paired_args: a string of optional arguments to pass to kallisto quant for performing pseudo-alignment to the list of synthetic ERCC-defined transcripts, for applicable experiments. kallisto_index_args: a string of optional arguments to pass to kallisto index when first preparing kallisto for a given reference transcriptome. salmon_index_args: a string of optional arguments to pass to salmon index when first preparing salmon for a given reference transcriptome. Note that Kallisto is used instead of Salmon, by default (see the --use_salmon option). salmon_quant_args: a string of optional arguments to pass to salmon quant when quantifying transcript abundances with Salmon. samtools_args: a string of optional arguments to pass to samtools mpileup before calling variants on human samples. star_args: a string of optional arguments to pass to STAR when aligning samples to the chosen reference genome. trim_adapter_args_single, trim_adapter_args_paired: these are passed to the ILLUMINACLIP settings for trimmomatic (after the adapter) for analysis with single and paired-end reads, respectively. Leave empty (i.e. \"\") to not perform adapter trimming. trim_quality_args: literal string arguments used for quality trimming- leave empty (i.e. \"\") to not perform quality trimming. An example value could be \"LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:75\". regtools_args: a string of optional arguments to pass to regtools junctions extract when quantifying junctions. wigToBigWig_args: a string of optional arguments to pass to wigToBigWig when producing coverage BigWig files from their wig counterparts. 8.2.3 Miscellaneous settings A couple configuration variables control functionality specific to SPEAQeasy (i.e. not external software tools called by SPEAQeasy). These are documented below. num_reads_infer_strand: The number of lines in each FASTQ file to subset when determining strandness for a sample. wiggletools_max_threads: The maximum number of files to handle simultaneously when producing wig coverage files. We have encountered issues with wiggletools starting too many threads (one is used per sample handled), and so this can be lowered if similar issues occur for other users in large experiments. "],["software.html", "9 Software 9.1 Software Versions 9.2 Docker Images 9.3 Using Custom Software", " 9 Software SPEAQeasy makes use of several external bioinformatics software tools. The pipeline additionally supports the use of these tools via docker containers- this section also documents the docker images used in this mode. 9.1 Software Versions Here is the full list of software used by this pipeline: Software Version Command used by the pipeline bcftools 1.10.2 bcftools fastQC 0.11.8 fastqc hisat2 2.2.1 hisat2, hisat2-build htslib 1.10.2 tabix java 8+ java kallisto 0.46.1 kallisto nextflow &gt;=0.27.0 (tested with 20.01.0) nextflow R 3.6 Rscript regtools 0.5.1 regtools RSeQC 3.0.1 bam2wig.py salmon 1.2.1 salmon samtools 1.10 samtools SubRead 2.0.0 featureCounts trimmomatic 0.39 java -jar path/to/trimmomatic.jar wiggletools 1.2.1 wiggletools wigToBigWig 4 wigToBigWig 9.2 Docker Images The following container versions are used in this pipeline. These are automatically managed by SPEAQeasy. Image Tag Software libddocker/r_3.6.1_bioc latest R and Bioconductor 3.10 libddocker/ubuntu16.04_base 1_v3 Ubuntu Base libddocker/infer_strandness latest R, Bioconductor 3.10, Kallisto libddocker/kallisto 0.46.1 Kallisto libddocker/fastqc 0.11.8 FastQC libddocker/trimmomatic 0.39 Trimmomatic libddocker/hisat2 2.2.1 HISAT2 libddocker/rseqc 3.0.1 RSeQC libddocker/samtools 1.10 Samtools libddocker/salmon 1.2.1 Salmon libddocker/regtools 0.5.1 Regtools libddocker/subread 2.0.0 SubRead/FeatureCounts libddocker/wiggletools-1.2 1_v3 Wiggletools libddocker/variant_calling 1.10.2 Samtools, BCFTools 9.3 Using Custom Software Some users may wish to desire to “swap out” software tools used by particular steps in the pipeline. For example, a researcher may wish to use Trim Galore! instead of Trimmomatic when trimming samples. SPEAQeasy doesn’t “officially support” Trim Galore currently, so this will require making some direct changes to code. We provide a guide below for researchers interested in modifying a component of SPEAQeasy to use a different software tool. Please note that a couple software alternatives are already supported (Salmon instead of Kallisto for psuedoalignment to a transcriptome via the --use_salmon option; STAR instead of HISAT2 for alignment to a genome via the --use_star option), and using these does not require any of the work described below. Additionally, the below guide requires “local” {installation](#installation) of SPEAQeasy (not using docker to manage software). 9.3.1 Prerequisites familiarity with basic glob expressions familiarity with basic shell scripting (if-statements, variables, and the mv command) knowledge about what output files are created from both the original and replacement software tool basic knowledge of the SPEAQeasy workflow the replacement software tool should be on the PATH 9.3.2 Modifying main.nf 9.3.2.1 Replace the relevant command We will begin by locating the relevant process in the file main.nf. While process names are intended to be intuitive, this step may require some inference. We are modifying the way SPEAQeasy trims files, so the process of interest is called “Trimming”, and we locate this around line 883 (the exact number may change). process Trimming { Next, we try to locate the command that runs the current software tool, Trimmomatic. Typically, there will only be a single command (possibly formatted to be spread across multiple lines for clarity). We look for this command in the section of the shell block of the process, enclosed in triple single-quotes. The below “template” version of the Trimming process may serve as a guide for where to look. Code that we can safely ignore has been replaced with brief comments. process Trimming { // [Some initial details] input: // [Nextflow channel for input files] output: // [Nextflow channel for output files] shell: // [Some code to set arguments to Trimmomatic] &#39;&#39;&#39; # [Code to determine whether to trim this sample] # Run trimming if required if [ &quot;$do_trim&quot; == true ]; then # [Set up some variables to pass to the Trimmomatic command] java -Xmx512M \\ -jar $trim_jar \\ !{trim_mode} \\ -threads !{task.cpus} \\ -phred33 \\ *.f*q* \\ !{output_option} \\ $adapter_trim_settings \\ !{params.trim_quality_args} else # [Rename files if this sample wasn&#39;t trimmed] fi # Write any output to a log cp .command.log trimming_!{fq_prefix}.log # [Pass some details to the SPEAQeasy-specific logs] &#39;&#39;&#39; } In the Trimming process, there is a quite a bit of bash code used to conditionally trim samples depending on the --trim_mode argument and FastQC results. We can keep most of this bash code exactly how it is. From above, the command to run Trimmomatic looks like this: java -Xmx512M \\ -jar $trim_jar \\ !{trim_mode} \\ -threads !{task.cpus} \\ -phred33 \\ *.f*q* \\ !{output_option} \\ $adapter_trim_settings \\ !{params.trim_quality_args} We will replace this code with something similar, but appropriate for Trim Galore. In general and vague terms, we want the replacement to look like this (brackets are used in place of the actual code or variable name to be implemented): [tool_name] [desired arguments] \\ --[core or thread argument] !{task.cpus} \\ [input files] For our particular case, the exact code can look like the segment below. Here we identify that the option controlling number of cores to use is -j and the input FASTQ files can be passed the same way as to Trimmomatic, via the *.f*q* glob. Additionally, Trim Galore requires that a sample should be explicitly specified as paired-end. We grab the value of the --sample argument with the syntax !{params.sample} (any option passed to SPEAQeasy can be referenced this way!), and use some bash code to conditionally add the --paired and --retain_unpaired arguments to Trim Galore (we will later cover why the latter is added). if [ &quot;!{params.sample}&quot; == &quot;paired&quot; ]; then paired_opt=&quot;--paired --retain_unpaired&quot; else paired_opt=&quot;&quot; fi trim_galore \\ -j !{task.cpus} \\ ${paired_opt} \\ *.f*q* 9.3.2.2 Rename output files Next, the goal is to rename files to match the names of the original outputs (in our case, those from Trimmomatic). This obviates the need for worrying about managing nextflow channels, and which files should be “published” to the output directory. In this case, SPEAQeasy specifies a particular output file naming style when using Trimmomatic. We can determine the naming rules from the below code (in particular, note that output_option variable): shell: file_ext = get_file_ext(fq_file[0]) if (params.sample == &quot;single&quot;) { output_option = &quot;${fq_prefix}_trimmed.fastq&quot; trim_mode = &quot;SE&quot; adapter_fa_temp = params.adapter_fasta_single trim_clip = params.trim_adapter_args_single } else { output_option = &quot;${fq_prefix}_trimmed_paired_1.fastq ${fq_prefix}_unpaired_1.fastq ${fq_prefix}_trimmed_paired_2.fastq ${fq_prefix}_unpaired_2.fastq&quot; trim_mode = &quot;PE&quot; adapter_fa_temp = params.adapter_fasta_paired trim_clip = params.trim_adapter_args_paired } From the above code and the documentation for Trim Galore 0.6.5, we can construct a table of all output file names (where “[id]” is sample-specific): Tool Pairing Output filename Trimmomatic single \"[id]_trimmed.fastq\" Trimmomatic paired \"[id]_trimmed_paired_1.fastq“,”[id]_unpaired_1.fastq“,”[id]_trimmed_paired_2.fastq“,”[id]_unpaired_2.fastq\" Trim Galore single \"[id]_trimmed.fq.gz\" Trim Galore paired \"[id]_1_val_1.fq.gz“,”[id]_1_unpaired_1.fq.gz“,”[id]_2_val_2.fq.gz“,”[id]_2_unpaired_2.fq.gz\" We first note that Trim Galore gzips its outputs by default. To produce uncompressed files (matching Trimmomatic), we’ll add the --dont_gzip argument to the trim_galore command. Altogether, the code should like as below. In the Trimming process, the sample ID is given by !{fq_prefix}. The file renaming occurs in the final if-statement. if [ &quot;!{params.sample}&quot; == &quot;paired&quot; ]; then paired_opt=&quot;--paired --retain_unpaired&quot; else paired_opt=&quot;&quot; fi trim_galore \\ --dont_gzip \\ -j !{task.cpus} \\ ${paired_opt} \\ *.f*q* # Rename files to imitate SPEAQeasy outputs from Trimmomatic if [ &quot;!{params.sample}&quot; == &quot;paired&quot; ]; then mv !{fq_prefix}_1_val_1.fq !{fq_prefix}_trimmed_paired_1.fastq mv !{fq_prefix}_2_val_2.fq !{fq_prefix}_trimmed_paired_2.fastq mv !{fq_prefix}_1_unpaired_1.fq !{fq_prefix}_unpaired_1.fastq mv !{fq_prefix}_2_unpaired_2.fq !{fq_prefix}_unpaired_2.fastq else mv !{fq_prefix}_trimmed.fq !{fq_prefix}_trimmed.fastq fi "],["help.html", "10 Help 10.1 Common Errors 10.2 Deeper Investigation 10.3 Resuming 10.4 No Internet Access? 10.5 Docker help", " 10 Help 10.1 Common Errors SPEAQeasy should be configured so that fundamental issues related to pipeline function do not arise. If you encounter an error and believe it to be a design flaw in SPEAQeasy, you can always submit a github issue. However, please take a look at the following common issues: A job/ process is not given enough memory or time: pipeline runs on large samples or datasets may require more memory or a higher time limit. When reported correctly, the pipeline will indicate an error status of 140 (for SGE or SLURM environments); however, memory issues can take many forms, and related error messages are not always clear. In this example case, the process PairedEndHISAT failed due to insufficient memory, but indicated a general error status (1): How Nextflow may report memory-related errors Attempt to provide the process more memory in your config. In this case the configuration for PairedEndHISAT looks like this (for SGE users): withName: PairedEndHISAT { cpus = 4 memory = &#39;20.GB&#39; clusterOptions = &#39;-l h_fsize=500G -R y&#39; } Note that disk space may also be the limitation (particularly for SGE users). See the configuration section for more info. Strandness for at least one sample disagrees with the asserted value: all samples for a particular pipeline run are expected to have the same “strandness”, specified by the --strand command option in your main script. SPEAQeasy infers the strandness for each sample to verify your data has its expected structure- by default, when any sample is determined to have a strandness different than what is specified, the pipeline halts with an error. This is designed to catch faulty data quickly before analysis, but also consider the --force_strand command option to suppress errors of this kind. Nextflow has trouble communicating with your cluster: a less common issue can occur on slower clusters, related to nextflow failing to poll your grid scheduler (like SGE or SLURM) for information it needs about the jobs that are running. This can show up in an error message like: ProcessPairedEndHISAT (Prefix: Sample_FE2P1_blood)terminated for an unknown reason -- Likely it has been terminated by the external system. We have found that raising the exitReadTimeout to a large value (such as 30 minutes) solves this issue, but consider raising it further if needed. executor { name = &#39;sge&#39; queueSize = 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;30 min&#39; } 10.2 Deeper Investigation If pipeline execution halts with an error, and the cause/solution does not appear to be described in the common issues, what can be done? This section provides information about next steps for debugging a potential problem. 10.2.1 Checking Per-Sample Logs At the end of each pipeline run (whether complete or after halting due to an error), SPEAQeasy generates logs to trace the exact series of steps each sample underwent. These are designed primarily as a debugging tool, especially to catch issues that may be sample-specific. Identifying the source of an error generally involves the following steps: Identify the failing process and sample name: the main output log from nextflow will be called “SPEAQeasy_output.log” by default (it is generally a very long text file). SPEAQeasy automatically compiles the most typically relevant information in this log; however, the error message (if applicable) at the bottom of the log is typically helpful for pinpointing the process and sample where the error occurred. Note: if a sample name is not given with the process name, the process involves all samples. Look for a message similar to the following: Main error message in SPEAQeasy_output.log Find the SPEAQeasy-generated log for the failing sample: logs are located under the output folder specified by the --output command flag- by default, this will be SPEAQeasy/results/logs/. Simply find the sample name given by step 1 (or pick any sample if no name was provided). Locate the process from step 1 in the log from step 2: because the processes are in order of reported submission, the failed process will typically be near the bottom of the log. The most immediately informative portion to look for is usually the logged output from executing that process. This is at the bottom between the dashed lines labelled BEGIN LOG and END LOG. Locating the appropriate process in your log Locating the output for the process If the log output does not make it clear what wrong, explore the following: Check the exit code: this is near the top for a given process. An exit code of 1 is often not very informative by itself (a generic error status), but other values can sometimes inform you of the exact issue. For example, the common exit status of 140 indicates that a process failed due to exceeding memory, disk, CPU, or time constraints (for SGE or SLURM clusters). Most frequently, memory is the limiting factor; consider raising the memory allocated for the process in the relevant configuration file. Check the content of the working directory: the path to the relevant working directory is given on the first line of the log for a given process. One immediate “red flag” could be a core dump (a file named something like core.[numbers]). These files are produced when a process unexpectedly terminates, which often can indicate the process needs more memory or disk space (see configuration). It is also worth checking for other log files (certain processes, like SingleEndHISAT and PairedEndHISAT, have output which is not printed between the BEGIN LOG and END LOG per-sample log sections). 10.3 Resuming An important feature of SPEAQeasy (because it is based on nextflow) is the ability to resume pipeline execution if an error occurs for any reason. To resume, you must add the -resume flag to your “main” script, determined here, and rerun. Otherwise, the default is to restart the entire pipeline, regardless of how much progress was made! 10.3.1 Trouble resuming? In some cases, nextflow will improperly determine which processes have finished, when a pipeline run halts with an error. This can be related to how your filesystem communicates with nextflow. We have found that specifying cache = 'lenient' in the process section of the config fixes issues with resuming (such as re-running a process which had actually completed). See nextflow’s documentation on this. In your config, this could look like: process { time = { 10.hour * task.attempt } errorStrategy = { task.exitStatus == 140 ? &#39;retry&#39; : &#39;terminate&#39; } maxRetries = 1 // this can be placed here or anywhere inside the &#39;process&#39; brackets starting // at the top of this code chunk cache = &#39;lenient&#39; withName: PullAssemblyFasta { cpus = 1 clusterOptions = &#39;-l mf=2G,h_vmem=2G&#39; } This is the default in jhpce.config, sge.config, and docker_sge.config, but may be needed on other cluster types as well. 10.4 No Internet Access? For users who do not have internet access when executing pipeline runs, you may first run bash scripts/manual_annotation.sh. This script must be run from the repository directory (from a machine with internet access). Modify the four lines in the “user configuration section” at the top of the script for your particular set-up. This sets up everything so that subsequent runs of the pipeline do not need an internet connection to complete. 10.5 Docker help For those who wish to use docker to manage SPEAQeasy software dependencies, we provide a brief set-up guide. Install docker A set of instructions for different operating systems are available on the Docker site. Create a docker group sudo addgroup docker Add user to docker group sudo usermod -aG docker &lt;your_user&gt; Checking installation Log out and log back in to ensure your user is running with the correct permissions. Test Docker installation by running: docker run hello-world "],["glossary.html", "11 Glossary", " 11 Glossary Cluster: shorthand for “performance computing cluster”- a network of many machines, which a member may utilize to interactively run or submit computational jobs. Most data analysis for genomics data is performed on a cluster, which collectively has far more memory and computing power than say, an individual’s laptop or desktop machine. Docker: Docker is a tool for creating, sharing, and running containers. Containers package up software and dependencies, so that a piece of software runs identically on any machine (capable of running docker containers). The idea is to avoid difficult-to-predict differences in software and operating system versions between machines, which often affect how a single program behaves. Docker image: An image describes the exact environment (operating system, installed programs, etc) needed to run a piece of software. Containers are the processes which run from a specified Docker image. For example, the libddocker/rseqc:3.0.1 docker image exists to run RSeQC 3.0.1- thus the image contains a particular version of Python and associated libraries, ensuring consistent execution on a different machine. ERCC: External RNA Controls Consortium, a group who developed a set of controls for sources of variability in RNA-seq expression data attributed to platform, starting material quality, and other experimental covariates. ERCC spike-ins are polyadenylated transcripts which can be added at known concentration to each sample. Quantifying these spike-ins during analysis can help calibrate RNA-seq results and adjust for technical confounders. SPEAQeasy provides the --ercc option for experiments using ERCC spike-ins. FASTA: a file format for storing nucleotide sequences in plain-text Features: genomic sites of interest- in the case of SPEAQeasy, features include genes, exons, exon-exon junctions, and transcripts. These features are described using GenomicRanges objects as part of the main RangedSummarizedExperiment outputs from SPEAQeasy. GTF: a file format for storing sequence ranges and associated annotation/information MD5 sum: a number (often displayed as a hexadecimal string) commonly used as a checksum to verify the integrity of files (such as after transferring a large file to a new filesystem). The samples.manifest file optionally can include MD5 sums for each FASTQ file, though SPEAQeasy does not verify file integrity. Module: in the context of SPEAQeasy and its documentation, a module refers to an Lmod environment module. Nextflow can be very simply configured to use modules to manage software, but this typically involves significantly more work unless the modules already exist on the system where SPEAQeasy will be run. Modules are “units of software” which can be loaded with a simple command, and potentially shared by many users. Nextflow: the workflow management language SPEAQeasy runs on, which organizes pieces of the pipeline into one coherent workflow that runs largely the same way for users of many different computer systems. Bookdown: an open-source R package we use to help generate and organize this documentation website, written in R markdown. RNA-seq: short for RNA sequencing- a high-throughput method for quantifying transcripts present in a sample or samples. Samples of mRNA are converted into cDNA libraries, at which point alignment to a reference genome and other analysis steps can proceed. RNA-seq is commonly used to quantify gene expression, though other information about the transciptome can be captured with RNA-seq (such as alternative splicing events). Stranded/strandness: the orientation of RNA reads during sequencing. Depending on the sequencing protocol, some reads may be oriented in the direction of the ‘sense’ strand of corresponding cDNA, or potentially some in the ‘antisense’ direction. Strand-specific protocols are described as either “forward” or “reverse”; protocols which do not have a specific read orientation are called “unstranded” within SPEAQeasy. "]]
